{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Training models in scikit-learn\n",
    "\n",
    "In this notebook we will show how we train models in scikit-learn using example of polynomial regression. We will learn how to\n",
    "* Split the data into training set and test set\n",
    "* Fit a polynomial model to the training data\n",
    "* Tune the the hyperparameter polynomial degree using cross-validation\n",
    "* Evaluate performance on the test set\n",
    "\n",
    "This tutorial complements the Lecture material\n",
    "__Machine Learning Basics__ \n",
    "__Part IV: Training Machine Learning Models__\n",
    "\n",
    "## Generate the data\n",
    "\n",
    "We will generate the training data from a quadratic model $y=2x^2+4x+5$ by generating random features from uniform distribution for the interval $[-3,3]$ using `np.random.rand` and adding Gaussian noise with $\\sigma=3$ to the target values using `np.random.randn`.\n",
    "\n",
    "Run the code bellow to generate your dataset. Note that\n",
    "* Feature matrix is given in the variable `X`\n",
    "* Target vector is given in variable `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# DEFINE THE TRUE MODEL\n",
    "\n",
    "def TrueModel(x):\n",
    "    return 2*x**2+4*x+5\n",
    "\n",
    "# GENERATE RANDOM SAMPLES\n",
    "\n",
    "# to keep the random samples same at every run\n",
    "gen = np.random.RandomState(40)\n",
    "# generate 100 random samples\n",
    "n=100\n",
    "X = 6*gen.rand(n)-3\n",
    "# generate noise for each sample\n",
    "noise = 3*gen.randn(n)\n",
    "# generate noisy samples\n",
    "y = TrueModel(X) + noise\n",
    "\n",
    "# convert features X into a 2D array\n",
    "X = X.reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity 1:__ Write code to print out the dimensions of the feature matrix X and target vector y. How many features and samples we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensions of feature matrix and target vector\n",
    "print('Feature Matrix:', None)\n",
    "print('Target Vector:', None)\n",
    "\n",
    "# Print number of features and samples \n",
    "print('Samples: ',None)\n",
    "print('Features: ',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity 2:__ Plot the generated samples. *Hint:* use `plt.plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data to training and test set\n",
    "\n",
    "Scikit learn offers a function `sklearn.model_selection.train_test_split` to perform the splitting of the dataset. A common pattern is to keep 80% of the data for training and use 20% for testing. This can be set using an option `test_size`. Splitting is random, so if we wish to keep the same split at every run we can use parameter `random_state`.\n",
    "\n",
    "Finally, to create a representative test set, we can use the `stratify` parameter. In this example we will split the data into 7 bins by rounding the feature values, and stratify the test set to have the same proportion of the data from each bin as the training set.\n",
    "\n",
    "__Activity 3:__ Play with parameter `test_size` to see different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create bins for the target values\n",
    "bins = np.round(X)\n",
    "\n",
    "# perform stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, stratify = bins, random_state=42)\n",
    "\n",
    "# plot the training and test data\n",
    "plt.plot(X_train,y_train,'o',label='Training set')\n",
    "plt.plot(X_test,y_test,'o',label='Test set')\n",
    "plt.legend()\n",
    "plt.title('Train test split')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a polymomial model\n",
    "\n",
    "Scikit-learn does not implement a polynomial regression model. Instead the polynomial regression is performed in two steps:\n",
    "1. Polynomial feature transformation - a __transformer__ object `PolynomialFeatures` transforms each feature $x$ into a feature vector $(1,x,x^2,...,x^M)$, where polynomial degree $M$ is defined by setting parameter `degree` \n",
    "2. Performing multivariate linear regression - a __regressor__ object `LinearRegression` fits the model \n",
    "$y=w_0x_0+x_1w_1+...+x_Mw_M$ to the data.\n",
    "\n",
    "Note that if we combine these two steps, we will have $x_j=x^j$ and therefore create a polynomial model $y=w_0+w_1x+...+w_Mx^M$\n",
    "\n",
    "The code bellow demonstrates how we can fit the 2nd order polynomial model to the training set and evaluate it on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create polynomial features object of second degree\n",
    "M=2\n",
    "poly_features = PolynomialFeatures(degree=M)\n",
    "\n",
    "# transform training features (note conversion to 2D array)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "# Create and fit multivariate linear regression model\n",
    "# We do not need intercept, because the first feature is 1\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train_poly,y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "r2_test = model.score(X_test_poly,y_test)\n",
    "print('R2 score on test set is ', round(r2_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the fitted polynomial model\n",
    "\n",
    "__Activity 4:__ Fill in the code bellow to plot the second order polynomial model that we just fitted. Then change the polynomial degree in the cell above, and rerun both cells to see how the model changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and test data\n",
    "\n",
    "\n",
    "\n",
    "# define the feature space\n",
    "X_curve=np.linspace(-3,3,100).reshape(-1, 1)\n",
    "\n",
    "# Perform the polynomial feature transformation of X_curve\n",
    "X_curve_poly = None\n",
    "\n",
    "# Predict target values using X_curve_poly\n",
    "y_curve = None\n",
    "\n",
    "# plot the curve\n",
    "\n",
    "\n",
    "# annotate the plot\n",
    "plt.title('Polynomial fit $M={}$'.format(M))\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a model pipeline\n",
    "\n",
    "It is not always convenient to perform several steps for model training, prediction and evaluation. In particular if the model becomes an input to another `sklearn` object, such as hyperparameter search using cross-validation that we will introduce below, the steps need to be unified in a single object.\n",
    "\n",
    "Scikit-learn implements a class `sklearn.pipeline.Pipeline` to join multiple __steps__ into one model. `Pipeline` implements methods `fit`, `predict`, `score` and others to offer a unified API with other sklearn objects.\n",
    "\n",
    "In the code bellow we will demonstrate how to join the polynomial feature transformation and multivariate linear regression into a single model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Polynomial regression model is created as a 'pipeline'\n",
    "# combining creation of features (1,x,x^2,...) followed\n",
    "# by multivariate linear regression\n",
    "pipeline = Pipeline((\n",
    "(\"poly_features\", PolynomialFeatures(degree=2)),\n",
    "(\"lin_reg\", LinearRegression(fit_intercept=False)) ))\n",
    "\n",
    "# Fit the model using features and labels\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# calculate performance\n",
    "r2_test = pipeline.score(X_test,y_test)\n",
    "print('R2 score on test set is ', round(r2_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in detail at the syntax for creating the pipelines. It is in the form `Pipeline(steps)`, where the input argument `steps` is a list of transforms and models to be chained, in the order with which they need to be called. Each step is described by a tuple `(name, model)`, where `name` is a string chosen by the user and `model` is a `sklearn` object.\n",
    "\n",
    "To access the original objects joint in a `Pipeline` we use attribute `Pipeline.named_steps`. For example, the linear regression object defined in the cell above can be accessed as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access LinearRegression object\n",
    "pipeline.named_steps[\"lin_reg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and coefficient of the linear regression $w_0,...,w_k$ can be accessed as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access coefficient of the LinearRegression object\n",
    "pipeline.named_steps[\"lin_reg\"].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have recovered a model $y=5+3.8x+1.9x^2$ which is fairly close to our true model.\n",
    "\n",
    "__Activity 5:__ Complete the code bellow to print out polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access polynomial degree of the PolynomialFeatures object\n",
    "pipeline.named_steps[None].degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Even if we stratify our test set, it is not guaranteed that it is representative and the performance may vary every time we create a different split. More robust way is to perform cross-validation, where we split the data into $k$ groups (folds), and each fold will be used to measure performance exactly once, while remaining data are used to fit the model. Average performance over the $k$ folds will be much more robust. If there are no hyperparameters to tune, cross-validation can be used directly to measure the performance of the model.\n",
    "\n",
    "In scikit-learn cross validation is called using `cross_val_score(estimator, X, y)` with arguments\n",
    "* `estimator:` the model to be fit\n",
    "* `X:` the feature matrix\n",
    "* `y:` optional target values or labels for supervised models\n",
    "- `cv:` optional argument that defines number of folds. \n",
    "\n",
    "The model returns an array with scores from each fold. \n",
    "\n",
    "__Activity 6:__ Perform cross-validation for polynomial models of different degrees and observe how the average $R^2$ score changes. To do that you can modify the polynomial degree and rerun the cell above multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "# define the model\n",
    "pipeline = Pipeline((\n",
    "(\"poly_features\", PolynomialFeatures(degree=2)),\n",
    "(\"lin_reg\", LinearRegression(fit_intercept=False)),))\n",
    "\n",
    "# perform 5-fold cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "\n",
    "# print scores\n",
    "print('Scores',np.around(scores,2))\n",
    "print('Cross-validated R2 score: mean {}, standard deviation {} '.format(round(scores.mean(),3),round(scores.std(),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search using cross-validation\n",
    "\n",
    "Finally we will show how we can automatically find the optimal hyperparameters in scikit-learn. One of the most common way is to train a model for each hyper-parameter value and measure its performance using cross-validation. This approach is implemented in `sklearn.model_selection.GridSearchCV`. \n",
    " \n",
    "First we need to create the grid search object using `GridSearchCV(estimator, param_grid)` with arguments:\n",
    "* `estimator:` the model to be fitted\n",
    "* `param_grid:` a dictionary, with parameter names as keys, and parameter values as lists\n",
    "* `cv:` optional parameter to determine the number of folds\n",
    "\n",
    "We will now find the optiomal polynomial degree automatically, using the `GridSearchCV`. First we need to define our parameter grid dictionary. Because we have a `Pipeline` object, the parameter name is combined of the step name `\"poly_features\"` and parameter name `\"degree\"` which are joined by two underscores `__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propose a range of possible values for the polynomial degree\n",
    "degrees = range(1,20)\n",
    "\n",
    "# define parameter dictionary\n",
    "# because the model is a pipeline, the name of the parameter is composed\n",
    "# of name of the step and parameters name joint by a double underscore\n",
    "parameters = {\"poly_features__degree\": degrees}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the `GridSearchCV` object and run the hyperparameter optimisation using method `fit`, which is performed on the training set. The chosen polynomial model can be accessed using variable `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create grid search object\n",
    "grid_search = GridSearchCV(pipeline, parameters,cv=5)\n",
    "\n",
    "# perform the search over all hyperparameter values\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# This is the chosen model\n",
    "best_model = grid_search.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the chosen polynomial, and the best R2 score identified through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree of the best model\n",
    "best_degree = best_model.named_steps['poly_features'].degree\n",
    "print('The best polynomial degree ',best_degree)\n",
    "\n",
    "# best score\n",
    "best_score = grid_search.best_score_\n",
    "print('The best cross-validated R2 score on training set is ',round(best_score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance on test set\n",
    "\n",
    "We can evaluate the performance on the test set directly using a trained `GridSearchCV` by calling its method `score`. It will return the score of the model with the best hyperparameter setting identified during the search and fitted to the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r2 = grid_search.score(X_test,y_test)\n",
    "print('R2 score on test set is: ', round(r2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "In this exercise you will find the best polynomial fit to noisy data.\n",
    "\n",
    "We will load the data 'noisy_data.csv' using `pandas`. The file contains two columns:\n",
    "* column zero contains features `X`\n",
    "* column one contains target values `y`\n",
    "\n",
    "Run the code bellow to create `X` and `y` and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# load data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/noisy_data.csv')\n",
    "X = df.values[:,0].reshape(-1,1)\n",
    "y = df.values[:,1]\n",
    "\n",
    "# print data\n",
    "plt.plot(X,y,'o')\n",
    "plt.title('Noisy data')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to fit a polynomial model to the data. Perform following steps:\n",
    "* Perform stratified train-test split \n",
    "* Create a polynomial model using `Pipeline`\n",
    "* Use `GridSearchCV` to find the optimal polynomial degree and print out the best cross-validated score\n",
    "* Calculate performance on the test set\n",
    "* Plot the fitted model\n",
    "* Print out the coefficients of the model\n",
    "\n",
    "Can you work out what was the underlying polynomial model? (*Hint: the coefficients were whole numbers*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRATIFIED TRAIN TEST SPLIT\n",
    "bins = np.round(X)\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE POLYNOMIAL MODEL\n",
    "pipeline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE PARAMETER GRID\n",
    "parameters = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMISE HYPERPARAMETERS\n",
    "# Create \n",
    "grid_search = None\n",
    "\n",
    "# Fit GridSearchCV object\n",
    "\n",
    "\n",
    "# extract the selected model\n",
    "best_model = None\n",
    "\n",
    "# print out best score and chosen polynomial degree\n",
    "print('The best cross-validated R2 score on training set is ',round(grid_search.best_score_,2))\n",
    "print('The best polynomial degree ',best_model.named_steps[\"poly_features\"].degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE PERFORMANCE ON TEST SET\n",
    "r2 = None\n",
    "print('R2 score on test set is ', round(r2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE FITTED MODEL\n",
    "\n",
    "# generate the feature space\n",
    "x_curve=None\n",
    "\n",
    "# predict the targets\n",
    "y_curve=None\n",
    "\n",
    "# plot the training data\n",
    "\n",
    "\n",
    "# plot the test data\n",
    "\n",
    "\n",
    "# plot the model curve\n",
    "\n",
    "\n",
    "# annotate\n",
    "plt.title('Polynomial fit')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COEFFICIENTS OF THE MODEL\n",
    "coef = None\n",
    "print('Coefficients: ',np.around(coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original model is $y=?$\n",
    "\n",
    "*Note: Splitting of training and test set is random, so different solutions are possible. From the shape of the dataset we can judge that the underlying polynomial was cubic. If you obtain polynomial of higher degree, try to re-run your code until you get a cubic polynomial. That will help you to work out true model coefficients.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
