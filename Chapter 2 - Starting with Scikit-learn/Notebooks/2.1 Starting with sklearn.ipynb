{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Basics\n",
    "## Starting with Scikit-learn\n",
    "\n",
    "In this notebook we will introduce the basics of `sklearn` application interface (API). \n",
    "\n",
    "This notebook accompanies **parts II** and **III** of the **Lecture 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/brain-volume.png\" width = \"150\" style=\"float: right;\"> \n",
    "## Regression\n",
    "\n",
    "This example demonstrates the __regressor__ object API.\n",
    "\n",
    "The file 'neonatal_brain_volumes.csv' contains gestational ages (GA) and brain volumes of premature babies. We will fit a `LinearRegression` model to predict the brain volumes from GA.\n",
    "\n",
    "### Prepare the data\n",
    "First we will import the file using the `pandas` package and check its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas package\n",
    "import pandas as pd\n",
    "# read file into a dataframe object\n",
    "df = pd.read_csv('datasets/neonatal_brain_volumes.csv')\n",
    "# print the first few lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will convert the data into a numpy array and create a feature matrix containing the column 'GA' and the target vector containing brain volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# convert dataframe object into a numpy array\n",
    "data = df.to_numpy()\n",
    "# Create the feature matrix and convert it to a 2D numpy array\n",
    "X = data[:,0].reshape(-1,1)\n",
    "print('Feature matrix X dimensions: ', X.shape)\n",
    "print('Number of samples: ', X.shape[0])\n",
    "print('Number of features: ', X.shape[1])\n",
    "# Create the target vector\n",
    "y = data[:,1]\n",
    "print('Target vector y dimensions: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "Now we select and create the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearRegression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Create the model\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "The next step is to fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the coeficients of the linear model \n",
    "$y=w_0+w_1x$\n",
    "which we fitted to the data as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0=model.intercept_\n",
    "print('w0: ', round(w0))\n",
    "w1=model.coef_[0]\n",
    "print('w1: ', round(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "The model can be evaluated by calling the function `score`. For regressors this function returns the $R^2$ score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate r2 score\n",
    "r2 = model.score(X,y)\n",
    "# Print the score\n",
    "print('R2 score: ', round(r2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on new data\n",
    "\n",
    "We are interested in visualising the model, we will therefore create a grid that samples the feature space. The code bellow will create 10 samples that span the values between minimum and maximum of the GA. Note we need a 2D array for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = np.linspace(np.min(X),np.max(X),10).reshape(-1,1)\n",
    "print('Feature space:\\n',np.around(X_model).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to predict the target values for these new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = model.predict(X_model)\n",
    "print('Predicted targets for the feature space:\\n',np.around(y_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result\n",
    "We plot the result using `matplotlib` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# plot the data\n",
    "plt.plot(X,y,'bo', alpha = 0.5, label = 'samples')\n",
    "\n",
    "# plot the model\n",
    "plt.plot(X_model,y_model,'k',label = 'model')\n",
    "\n",
    "# Annotate the plot\n",
    "plt.title('Regression')\n",
    "plt.xlabel('Feature: Gestational age at scan')\n",
    "plt.ylabel('Target value: brain volume in mL')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "It is now your turn to write the solution to a following problem: You would like to predict GA of a preterm baby from the measurement of the brain volume. Note that in this case the GA and volumes switched the roles - volume is a feature and GA is the target value. The feature matrix `X1` and target vector `y1` were created for you.\n",
    "\n",
    "Write code to\n",
    "* Create the `LinearRegression` model\n",
    "* Fit the model \n",
    "* Calculate the $R^2$ score\n",
    "\n",
    "Commands for printing out score and the equation of the fitted model were created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrix using brain volumes\n",
    "X1 = data[:,1].reshape(-1,1)\n",
    "\n",
    "# Create the target vector using GA\n",
    "y1 = data[:,0]\n",
    "\n",
    "# Create the model\n",
    "model1 = None\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2_1 = None\n",
    "\n",
    "# Print the score\n",
    "print('R2 score: ', round(r2_1,2))\n",
    "\n",
    "# Print the equation of the fitted model\n",
    "print('Fitted model: y={}+{}x'.format(round(model1.intercept_),round(model1.coef_[0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "<img src=\"pictures/HeartSegmentation.gif\" width = \"150\" style=\"float: right;\">\n",
    " \n",
    "This example demonstrates the __classifier__ object API.\n",
    "\n",
    "The file 'heart_failure_data.csv' contains features Ejection Fraction (EF), Global Longitudinal Strain (GLS) and a label indicating whether patient has heart failure (HF). We will fit a `Perceptron` model to predict the heart failure from EF and GLS.\n",
    "\n",
    "### Prepare the data\n",
    "First we will import the file using the `pandas` package and check its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fine into a dataframe object\n",
    "df = pd.read_csv('datasets/heart_failure_data.csv')\n",
    "# print the first few lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code bellow creates the feature matrix `X` and label vector `y`. Note that now the feature vectors are 2-dimensional. Also, we need to scale the features to have zero mean and unit variance accross the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create and object to scale the features\n",
    "# to have zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# convert to numpy\n",
    "data = df.to_numpy()\n",
    "# create feature matrix containing EF and GLS\n",
    "X = scaler.fit_transform(data[:,:2])\n",
    "print('Feature matrix X dimensions: ', X.shape)\n",
    "# create label vector containing HF\n",
    "y = data[:,2]\n",
    "print('Target vector y dimensions: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "This code creates the `Perceptron` model. Note that we need to set the number of iterations for the fitting procedure because `sklearn` default does not work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "# Create the model\n",
    "model = Perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "This code fits the `Perceptron` model to the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients of the fitted decision function\n",
    "$h(\\mathbf{x})=w_0+w_1x_1+w_2x_2$ can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0=model.intercept_[0]\n",
    "print('w0: ', round(w0))\n",
    "w1=model.coef_[0][0]\n",
    "print('w1: ', round(w1))\n",
    "w2=model.coef_[0][1]\n",
    "print('w2: ', round(w2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "For classification models the function `score` returns accuracy, which is the proportion of the correctly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = model.score(X,y)\n",
    "# Print the score\n",
    "print('Accuracy score: ', round(accuracy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the model\n",
    "The result of the classification is plotted bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot data\n",
    "plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.75,label = 'Healthy')\n",
    "plt.plot(X[y==1,0],X[y==1,1],'r*',alpha=1,label = 'Heart Failure')\n",
    "\n",
    "# Plot decision boundary\n",
    "# Define y-coordinates\n",
    "x2 = np.array([X[:,1].min(), X[:,1].max()])\n",
    "# Define x-coordinates\n",
    "x1 = -(w0 + w2*x2)/w1\n",
    "# Plot \n",
    "plt.plot(x1, x2, \"k-\") \n",
    "\n",
    "plt.legend()\n",
    "plt.title('Classification')\n",
    "plt.xlabel('Feature 1: Ejection Fraction')\n",
    "plt.ylabel('Feature 2: Global Longitudinal Strain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Write a solution to a following problem: You would like find out whether using only Ejection Fraction (EF) would be suffiecient to predict the heart failure (HF). \n",
    "\n",
    "Write code to\n",
    "* Create the new feature matrix and the target vector\n",
    "* Fit the model and calculate the accuracy score\n",
    "* Print the equation of the decision boundary\n",
    "\n",
    "What is the drop in accuracy compared to using both features (EF and GLS)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature matrix containing EF only\n",
    "X2 = None\n",
    "\n",
    "# create label vector containing HF\n",
    "y2 = None\n",
    "\n",
    "# Create the model\n",
    "from sklearn.linear_model import Perceptron\n",
    "model2 = Perceptron(eta0=0.2)\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy2 = \n",
    "\n",
    "# Print the score\n",
    "print('Accuracy score is: ', round(accuracy2,2))\n",
    "\n",
    "# Print the decision boundary\n",
    "print('Decision boundary: {} {}x1 = 0'.format(round(model2.intercept_[0],2),round(model2.coef_[0][0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "<img src=\"pictures/T1.png\" width = \"150\" style=\"float: right;\">\n",
    "\n",
    "This example demonstrates the __clusterer__ object API.\n",
    "\n",
    "The file 'T1.png' contains a slice of T1-weighted magnetic resonance image (MRI) of the adult brain. The non-brain tissues have been removed in pre-processing. We perform `KMeans` clustering to segment white matter (WM), grey matter (GM) and cerebro-spinal fluid (CSF) in this image. \n",
    "\n",
    "\n",
    "### Prepare the data\n",
    "First we will load the image using the `matplotlib` function `imread` and display it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  image\n",
    "T1 = plt.imread('datasets/T1.png')\n",
    "\n",
    "# display image\n",
    "plt.imshow(T1, cmap = 'gray')\n",
    "\n",
    "# print shape\n",
    "print('Image dimensions: ', T1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to convert the image into the feature matrix suitable for processing using `sklearn` functions. First we need to remove the background pixels that have values zero. Then we need to create the feature matrix as a 2D array object, but with only one feature in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the non-zero elements\n",
    "ind = T1>0\n",
    "# Create the feature matrix with the correct dimensions\n",
    "X = T1[ind].reshape(-1,1)\n",
    "print('Shape of feature matrix X is ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit the model\n",
    "\n",
    "Now we are ready to perform k-means clustering into 3 classes, which will correspond to three brain tissues: WM, GM and CSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create the model\n",
    "model=KMeans(n_clusters=3, random_state = 42)\n",
    "# Fit the model\n",
    "model.fit(X)\n",
    "# Fitted parametres\n",
    "c = model.cluster_centers_\n",
    "print(c.round(1).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the labels\n",
    "\n",
    "The next step is the predict the labels. Note that this time we did not calculate any score - this is because we do not have the training labels, so cannot evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels\n",
    "y=model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result\n",
    "To plot the result, we need to reshape the predicted labels to the original 2D array and then we can display it as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty segmentation image\n",
    "segmentation = np.zeros(T1.shape)\n",
    "# Paste the labels into correct locations\n",
    "segmentation[ind] = y+1\n",
    "# Plot the segmentation\n",
    "plt.imshow(segmentation, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Now perform the k-means clustering for the T2-weighted image 'T2.png'. Is the result similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load  image\n",
    "T2 = plt.imread('datasets/T2.png')\n",
    "\n",
    "# select non-zero pixels\n",
    "ind2 = T2>0\n",
    "\n",
    "# create feature matrix\n",
    "X2 = T2[ind2].reshape(-1,1)\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model2=None\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "\n",
    "# Predict the labels\n",
    "y2=None\n",
    "\n",
    "# create segmentation image\n",
    "segmentation2 = None\n",
    "\n",
    "\n",
    "# Plot the segmentation\n",
    "plt.imshow(segmentation2, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "<img src=\"pictures/malignant.gif\" width = \"150\" style=\"float: right;\">\n",
    "\n",
    "This example demonstrates the __transformer__ object API.\n",
    "\n",
    "The breast cancer dataset is in-built in `sklearn` and it contains 30 features - properties of cells extracted using biopsy and photographed under a microscope - and labels whether the tumour was malignant or benign.\n",
    "\n",
    "We will reduce the dimensionality of the feature vectors to 2 to visualise the patterns in this high-dimensional dataset.\n",
    "\n",
    "\n",
    "### Prepare the data\n",
    "\n",
    "First we load the dataset and check it's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "print(bc.keys())\n",
    "print('\\n Features: \\n', bc.feature_names)\n",
    "print('\\n Labels: ', bc.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the feature matrix and scale it using standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=bc.data\n",
    "print('We have {} features.'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "We will choose the principal component analysis with 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model = PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model \n",
    "\n",
    "The model is fitted using function `fit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X)\n",
    "pc1 = model.components_[0]\n",
    "pc2 = model.components_[1]\n",
    "print('Component 1: \\n',np.around(pc1,1))\n",
    "print('Component 2: \\n',np.around(pc2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the features\n",
    "\n",
    "Rather than predicting some outputs, PCA transforms the features using the function `transform`. We can check that transformed feature vectors are now 2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = model.transform(X)\n",
    "print('We have {} features.'.format(X_reduced.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "The code below visualises the projection of the breast cancer data on the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = bc.target\n",
    "\n",
    "plt.plot(X_reduced[:, 0][Labels==0], X_reduced[:, 1][Labels==0], \"r*\", alpha = 0.5, label = 'malignant')\n",
    "plt.plot(X_reduced[:, 0][Labels==1], X_reduced[:, 1][Labels==1], \"bo\", alpha = 0.5, label = 'benign')\n",
    "\n",
    "plt.title('Dimensionality reduction')\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Compare performance of a `Perceptron` classifier to detect breast cancer using the original and reduced features.\n",
    "\n",
    "First we will load the dataset and extract the feature matrix and label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the data\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "# Original dataset - feature matrix and label vector\n",
    "X=bc.data\n",
    "y=bc.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to apply PCA to the feature matrix and check that the reduced matrix has only two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA model with 2 components\n",
    "model = None\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "\n",
    "# Transform the feature matrix to 2-dimensional space\n",
    "X_reduced = None\n",
    "\n",
    "# Print number of features\n",
    "print('We have {} features.'.format(X_reduced.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compared the accuracy of classification using `Perceptron` when fitting to the original feature matrix `X` or reduced feature matrix `X_reduced`. Note that labels vector `y` is the same in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create Perceptron model\n",
    "clf = Perceptron()\n",
    "\n",
    "# Fit model using the original dataset\n",
    "clf.fit(scaler.fit_transform(X),y)\n",
    "\n",
    "# Calculate accuracy using the original dataset\n",
    "acc_orig = clf.score(scaler.fit_transform(X),y)\n",
    "print('Original dataset accuracy: ',round(acc_orig,2))\n",
    "\n",
    "# Fit model using the reduced dataset\n",
    "\n",
    "\n",
    "# Calculate accuracy using the reduced dataset\n",
    "acc_reduced = None\n",
    "print('Reduced dataset accuracy: ',round(acc_reduced,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
