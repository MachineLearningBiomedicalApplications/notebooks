{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "This week we will learn about weak learners: specifically, decision stumps and decision trees. This will provide us with the building blocks that we will need for ensemble learning (week 7)\n",
    "\n",
    "The basic idea behind decision trees is to stack a series of *weak* learning models one on top of another with a view to learning a single strong, generalisable, model. For example consider a simple model of diabetes:\n",
    "\n",
    "<img src=\"imgs/tree.png\" style=\"max-width:100%; width: 50%; max-width: none\">\n",
    "\n",
    "\n",
    "Each node in the tree asks a simple question of the data. If the response for a given example is true, its passes down one branch of the tree, if it is false, it passes down the other branch. Each weak learner in a tree is known as **Decision Stump**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": false
   },
   "source": [
    "## Decision Stumps\n",
    "\n",
    "Learning rules for tree stumps may take many forms including linear and non-linear fits (see figures b and c below); however, most often, classification trees nodes implement axis-aligned learning rules (figure a), which make predictions by thresholding on a single feature:\n",
    "\n",
    "<img src=\"imgs/weakleaners.png\" style=\"max-width:100%; width: 70%; max-width: none\">\n",
    "\n",
    "This is the approach taken in Scikit Learn. The choice of threshold is optimised so as to maximally split the different classes of the data. By example, try different ```thresholds``` below so as to maximally separate the classes ```{0,1}``` of the below  ```dataset```. Note, ```dataset``` represents a set of examples with **single feature** (first column); the second column represents the class. The below code seeks to set all data examples strictly below the threshold to the first group and all examples above to the second group; select an appropriate threshold and run the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.asarray([[1,0],\n",
    "           [2,0],\n",
    "           [3,0],\n",
    "           [4,0],\n",
    "           [5,0],\n",
    "           [6,1],\n",
    "           [7,1],\n",
    "           [8,1],\n",
    "           [9,1],\n",
    "           [10,1]])\n",
    "\n",
    "# To DO - try different threshold values\n",
    "threshold=None\n",
    "\n",
    "group1labels=[]; group2labels=[]\n",
    "\n",
    "for row in dataset:\n",
    "        # if the value of this feature for this row is less than the (threshold) value, \n",
    "        # split into left branch, else split into right\n",
    "        if row[0] < threshold:\n",
    "            group1labels.append(row[1])\n",
    "        else:\n",
    "            group2labels.append(row[1])\n",
    "            \n",
    "print('With threshold {} the group1labels are {}'.format(threshold, group1labels))\n",
    "print('With threshold {} the group2labels are {}'.format(threshold, group2labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biomedical example of such a decision stump is shown for diabetes classification in the figure below. Here, a decision is made by optimising a threshold on blood glucose levels. All examples with blood glucose less than $12mmol/L$ are considered healthy and pass down the left branch. All remaining examples are labelled as diabetic and pass down the right. It is expected that, for most problems, such a simple learning rule will result in many misclassified examples. Therefore, the role of subsequent decision stumps, in the tree, is to further refine the prediction.\n",
    "\n",
    "<img src=\"imgs/decisionstump.png\" style=\"max-width:100%; width: 30%; max-width: none\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weak Learning rules for classification\n",
    "\n",
    "We will discuss two options for classification tree cost functions; these are Information Gain and the Gini Index.\n",
    "\n",
    "- ***Information Gain*** represents the decrease in entropy obtained after a dataset is split on an attribute.\n",
    "\n",
    "- ***Gini Index*** reflects how mixed the classes are following the split. Perfect separation results in a score of 0, whereas the worst case split (that results in 50/50 classes in each group) results in a Gini score of 0.5 (for a 2 class problem).\n",
    "\n",
    "The calculation for ***Information Gain*** is as follows:\n",
    "\n",
    "$$ I(S_j,\\theta_j) = H(S_j) - \\sum_{i \\in {L,R}} \\frac{|S_j^i|}{|S_j|}H(S^i_j) $$\n",
    "\n",
    "Where $H(S_j)$ represents entropy (or the amount of disorder in a system): \n",
    "\n",
    "$$H(S_j)=\\sum_{y_k \\in Y} p(y_k) log_2 p(y_k); $$\n",
    "\n",
    "Y are the class labels (i.e. {0,1} for a binary problem); $ p(y_k)$ is the proportion of examples that have class $k$ reaching the current (in this case parent) node; $|S_j|$ is the total number of examples reaching node $j$ and $|S_j^i|$ is the number of examples passing down branch $i$ from node $j$.\n",
    "\n",
    "Looking at a toy example:\n",
    "\n",
    "<img src=\"imgs/InformationGainexample.png\" style=\"max-width:100%; width: 70%; max-width: none\">\n",
    "\n",
    "Here, we start with 23 examples at the parent node (14 o and 9 +). We are interested in testing a split that results in the right child node taking 11 instances (4 o and 7 +) and the left child node taking 12 instances (10 o and 2+). We calculate the entropies for the parent node and each child node as follows:\n",
    "\n",
    "\n",
    "<img src=\"imgs/InformationGainexample2.png\" style=\"max-width:100%; width: 70%; max-width: none\">\n",
    "\n",
    "The final cost is then estimated from a weighted sum of entropies from each child $H(S^i_j)$:\n",
    "\n",
    "$$ \\frac{11}{23}  \\times 0.946 + \\frac{12}{23}\\times 0.650 =0.792 $$\n",
    "\n",
    "Subtracted from the original parent entropy ($H(S_j)$):\n",
    "\n",
    "$$ ùêº(ùëÜ_ùëó,\\theta_ùëó )= 0.966‚àí0.792=0.163 $$\n",
    "\n",
    "Where the weights here are estimated from the proportion of examples in each node relative to that in the parent node i.e. ($\\frac{|S_j^i|}{|S_j|}$).\n",
    "\n",
    "The ***Gini Index*** is estimated as :\n",
    "\n",
    "$$Gini = 1 -\\sum_{y_k\\in Y} p(y_k)^2 $$\n",
    "\n",
    "Where, again, $p(y_k)$ represents the proportion of examples that have class $k$ reaching the node. Thus, estimating this for each child node separately:\n",
    "\n",
    "<img src=\"imgs/Gini_example.png\" style=\"max-width:100%; width: 70%; max-width: none\">\n",
    "<a id='gini'></a>\n",
    "And once again left and right splits are combined as a weighted sum:\n",
    "\n",
    "$$ I(S_j,\\theta_j) = \\sum_{i \\in {L,R}} \\frac{|S_j^i|}{|S_j|}Gini_i $$\n",
    "\n",
    "Which for this example returns:\n",
    "\n",
    "$$ ùêº(ùëÜ_ùëó,\\theta_ùëó )= \\frac{11}{23} \\times 0.463 + \\frac{12}{23} \\times 0.278 =0.366 $$\n",
    "\n",
    "Note, for datasets containing many categorical variables Information Gain is biased in favour of attributes with more categories; thus, Gini index is default for Scikit-Learn.\n",
    "\n",
    "Further, remember that when optimising costs, ***Gini Index*** must be ***MINIMISED*** and ***Information Gain*** must be ***MAXIMISED***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: building a decision stump classifier from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will go step by step through the process of building and training a decision stump to perform classification. This will build towards Exercise 2 (optional) where multiple decision stumps are stacked together into a tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the exercise we will use the Gini Coefficient to identify the best split of the data for one node in our tree. To test this we will use the following toy dataset $\\mathbf{X}$ made up of ten examples (rows) each with two features (columns 1 and 2) and binary labels (column 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([[2.771244718,1.784783929,0],\n",
    "           [1.728571309,1.169761413,0],\n",
    "           [3.678319846,2.81281357,0],\n",
    "           [3.961043357,2.61995032,0],\n",
    "           [2.999208922,2.209014212,0],\n",
    "           [7.497545867,3.162953546,1],\n",
    "           [9.00220326,3.339047188,1],\n",
    "           [7.444542326,0.476683375,1],\n",
    "           [10.12493903,3.234550982,1],\n",
    "           [6.642287351,3.319983761,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the process step by step editing the below functions, and testing it on this data, until we are sure our code functions correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 1.1: calculate the gini coefficient for a given split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "\n",
    "First we need to write our own function for evaluating the cost for any proposed split of the data using the Gini Coefficient\n",
    "\n",
    "$$Gini = 1 -\\sum_{y_k\\in Y} p(y_k)^2 $$\n",
    "\n",
    "In the below function the input will take a list (```branch```) with the total number of examples for each class in the split. Since we have two classes the length of the list must be two and given 10 data examples the sum of this list must be 10. Thus one possible example might be the list  ```[2 8]```). \n",
    "\n",
    "Thus we must calculate the gini coefficient by first estimating the total number of examples reaching this tree node. Then estimate the proportions of each class ($p(y_k$) and use this to estimate the coefficient.\n",
    "\n",
    "Complete the function by replacing all ```None ``` statements with the correct code. \n",
    "\n",
    "1. Sum the elements of ```branch``` to estimate total number of examples (```spilt_size```, line 14)\n",
    "2. Estimate ($p(y_k$): the proportion of total items in the split that belong to each class (line 19)\n",
    "3. Subtract $p(y_k)^2$ from the current estimate of the Gini Coefficient (line 20). See how here we have initialised our Gini as 1 outside of the loop; we can then iteratively subtract the proportion for each class from this total using the shorthand ```-=``` notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "<details>\n",
    "  <summary>Click me for hints </summary>\n",
    "\n",
    "    1. to estimate the total number of examples in a list you might use np.sum \n",
    "    2. to estimate ($p(y_k$)) take the specific example [2, 8] \n",
    "       - for this given list the total items is 10 and we have 2 elements of class 0 and 8 elements of class 1\n",
    "    3. As Gini is initialised to 1 we can achieve $gini=(1- \\sum_{y_k \\in Y} p(y_k)^2)$ by subtracting the proportion (for each class) estimate for each iteration of the loop\n",
    "    \n",
    "<details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_coefficient(branch):\n",
    "    \n",
    "    \"\"\"\n",
    "        Estimates Gini Coefficient for a given class split\n",
    "        input:\n",
    "            split: list of length k (where k= number of classes).\n",
    "                   The values at each index reflect the toal number of instances \n",
    "                   of each class, for this proposed branch split\n",
    "                             \n",
    "        output:\n",
    "            gini: gini coefficient for this split \n",
    "    \"\"\"\n",
    "    # estimating total number of samples in branch split (by summing contents of split list)\n",
    "    split_size=None\n",
    "    gini=1\n",
    "    # iterating over all items in the array\n",
    "    for class_total in branch:\n",
    "        # estimating p*p for this class label; subtracting from current gini total\n",
    "        proportion_class_k=None\n",
    "        gini-=None\n",
    "        \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test assume the split of data in our branch is [2,8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_coefficient([2,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 1.2: Propose splits \n",
    "<a id='step2'></a>\n",
    "\n",
    "\n",
    "Now that we are able to estimate the cost of any proposed split we need to be able to generate suggested splits of our dataset. For this we need a function, that given a) some feature to split on (variable ```index``` below = feature/column index) and b) ```value``` = some threshold to split on,  will \n",
    "1. check all the values of the features at that indexed position, and \n",
    "2. split the data into a left branch (if that data example's feature is below the threshold) and into a right branch (if that data example's feature is below the threshold). We will call this function ```test_split```.\n",
    "\n",
    "Edit the code here to input an ```if``` statement that checks for each row whether the value of the feature (indicated by the position variable ```index```) is below or above the threshold ```value```. If the feature value is below the threshold then add the row to the '```left```' list; if it is below add it to the '```right```' list. This will require four lines of code (from line 24)\n",
    "\n",
    "**Note that:***\n",
    "\n",
    "- ```for row in dataset:``` will slice rows from ```dataset``` so what we are asking you to code is a check against the feature value for the feature located at ``index``` . \n",
    "- you need to subsequently choose (based on that threshold) whether you add that row to the ```left``` list or the ```right``` list. How do you add items to a list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "<details>\n",
    "  <summary>Click me for hints </summary>\n",
    "\n",
    "    1. for a given row, how do you return the value of the feature at a column position given by 'index'\n",
    "    2. to add to a list you might want to use 'append'\n",
    "    \n",
    "<details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(index, value, dataset):\n",
    "    \"\"\"\n",
    "        Split a dataset based on an attribute and an attribute value \n",
    "        input:\n",
    "            index = feature/attribute index (i.e. data column index) on which to split on\n",
    "            value = threshold value (everything below this goes to left split, \n",
    "                    everything above goes to right)\n",
    "            dataset = array (n_samples,n_features+1) \n",
    "                    rows are examples \n",
    "                    last column indicates class membership\n",
    "                    remaining columns reflect features/attributes of data\n",
    "                             \n",
    "        output:\n",
    "            left,right: data arrays reflecting data split into left and right branches, respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    # create empty list that you will populate with rows of dataset \n",
    "    left=[]\n",
    "    right = []\n",
    "    # the loop below will slice rows from data set\n",
    "    for row in dataset:\n",
    "        # Ex 1.2: if the value of this feature for this row is less than the (threshold) value\n",
    "        # split into left branch, else split into right\n",
    "        None # replace with FOUR lines of code!\n",
    "\t\n",
    "    return np.asarray(left), np.asarray(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do** Let's now estimate a split for the \n",
    "\n",
    "1. ***first*** feature.  \n",
    "2. Estimate a threshold corresponding to the value of this feature for 7th data example - note\n",
    "    - ```rowindex```= 7th row; \n",
    "    - value corresponds to sampling from our data matrix $\\mathbf{X}$ for this row, column combination) \n",
    "    - ***remember in both cases that python indexes from zero!!***\n",
    "2. Look at $\\mathbf{X}$ - is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=None\n",
    "rowindex=None\n",
    "threshold=None\n",
    "print('the value of the feature {} at row {} of the data set is {}'.format(index,rowindex,threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus now estimating the split of the data by thresholding on this value gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches=test_split(index, X[rowindex,index], X)\n",
    "\n",
    "print('Our left branch is \\n {}'.format(branches[0]))\n",
    "print('Our right branch is \\n {}'.format(branches[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, branches here is a 2 object tuple, with the first object reflecting the left branch and the second object reflecting the right branch. Note, that this specific notation will be useful for the functions that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 1.3: Estimate total cost of split\n",
    "\n",
    "\n",
    "So in step 1 we calculate the Gini coefficient for one branch of a split. Let's call this $Gini_i$ (taken notation from [above](#gini)), and in step 2 we defined a function to propose a potential split of the data. \n",
    "\n",
    "We now need a function that will estimate the Gini coefficient for both branches of the split and combine to give a final cost:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(S_j,\\theta_j) && =  && \\sum_{i \\in {L,R}}  && \\frac{|S_j^i|}{|S_j|} && Gini_i \\\\\n",
    "&&=  && \\sum_{i \\in {L,R}} && \\frac{total\\_examples\\_branch_j}{total\\_examples\\_node} && Gini_i \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The below function:\n",
    "- estimates $|S_j|$ - the total number across both branches. \n",
    "- loops over each branch to estimate the gini cost of each branch;\n",
    "- then sums the result weighted by the proportion of data in that branch relative to the total reaching the node.\n",
    "\n",
    "Let's implement this function. \n",
    "\n",
    "***To Do***\n",
    "\n",
    "1. Calculate ```branch_per_class ```: the total number of data examples for a given ```branch``` with class =```class_val`` (line 31)\n",
    "   - slice all rows from ```branch``` that have class=```class_val``\n",
    "   - the class labels are stored in the final column of the data matrix\n",
    "2. count the total number of rows (```total_rows``` with this class (line 33) \n",
    "3. Use this to estimate the gini coefficient for that branch - save to variable ```gini_split``` (line 37, using the function estimated from step 1) \n",
    "4. calculate  ```weighted_by_sample_size``` - weight this by the proportion of samples that pass down this branch  (line 39) . \n",
    "\n",
    "This is then summed over loops (line 40) to estimate gini coefficient for this branch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "<details>\n",
    "  <summary>Click me for hints </summary>\n",
    "\n",
    "    1. to return all rows in ```branch ``` corresponding to a specific ```class_val```  you need to consider the last column ```branch[:,-1]``` and slice all rows where this equals ``class_val```\n",
    "    \n",
    "<details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cost(split,classes): \n",
    "    \n",
    "    \"\"\"\n",
    "        Estimates the cost for a proposed split \n",
    "        input:\n",
    "            splits: tuple or form (L,R) where L reflects the data for the left split and\n",
    "                    R reflects data for left split\n",
    "            classes: list of class values i.e. [0,1]\n",
    "                             \n",
    "        output:\n",
    "            cost: sum of gini coefficient for left and right sides of the split\n",
    "    \"\"\"\n",
    "    cost=0\n",
    "    total_samples=0\n",
    "    \n",
    "    #¬†estimate the relative size of each branch\n",
    "    for branch in split:\n",
    "        total_samples+=branch.shape[0]\n",
    "    \n",
    "    # for each (left/right) split on the proposed tree\n",
    "    for br_index,branch in enumerate(split):\n",
    "        # initialise list of class counts for this branch\n",
    "        class_counts_for_branch=[]\n",
    "        # for each class value, count total of data examples (rows) that have for this class, in this branch \n",
    "        for class_val in classes:\n",
    "            \n",
    "            if branch.shape[0] == 0: # don't continue if size of split is 0\n",
    "                continue\n",
    "           \n",
    "            # slice data to return only rows from branch which have this specific class value  \n",
    "            branch_per_class=None\n",
    "            # count the number of rows in for this class in this branch and append \n",
    "            total_rows=None\n",
    "            class_counts_for_branch.append(total_rows)\n",
    "\n",
    "        # estimate the gini coefficient for this split \n",
    "        gini_split=None\n",
    "        # estimated the weighted contribution for this split \n",
    "        weighted_by_sample_size=None\n",
    "        cost+=weighted_by_sample_size\n",
    "                        \n",
    "        \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the total cost of the split proposed from step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values=[0,1]\n",
    "splitcost=split_cost(branches,class_values)\n",
    "\n",
    "print('The cost of the proposed split is: ', splitcost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    "###  Exercise 1.4: Choose optimal feature/threshold split \n",
    "\n",
    "<a id='step4'></a>\n",
    "\n",
    "Finally, we now need to put this all together by looping through all possible features (all but the last column of our data matrix), and all possible thresholds to determine the best split for this node.\n",
    "\n",
    "The **output of this function is a dictionary** (see return statement). This saves all variables required for later predicting on that node, specifically: 1) the feature ```index``` that the node is split on; 2) the threshold ```value``` on which the data is split; and 3) the tuple of data arrays reflecting the resulting split (```branches```)\n",
    "\n",
    "Edit the below function to:\n",
    "  \n",
    "1. edit line 28 to loop over all features (all columns of the dataset, except the last, which instead reflects the class). **hint** define correct range of values  \n",
    "2. for each index, try proposing splits thresholds from the value of features at each row (line 33) \n",
    "  - **hint** look back over the prevous exercises - which one proposes splits of the data? What arguments does it require \n",
    "  - **hint** line 31 iterates over rows of the data set - how can you use this to propose a threshold value? \n",
    "  - **note** the output ```branches``` will be a tuple containing two data arrays (corresponding to the left and right branches for that split)\n",
    "\n",
    "\n",
    "3. Given ```branches ``` estimate the ```cost``` of the proposed split (line 35, **hint** see Ex 1.3)\n",
    "4. write an if statement that updates the variables ```best_cost ```, ```best_split ```,  ```best_index ``` and  ```best_value ```  _provided_ that ```cost```  is better than the one held previously (lines 36-40)\n",
    "  - **hint** the cost here is the Gini Index - should it be maximised or minimised?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_split(dataset):\n",
    "    \"\"\"\n",
    "        Search through all attributes and all possible thresholds to find the best split for the data\n",
    "        input:\n",
    "            dataset = array (n_samples,n_features+1) \n",
    "                    rows are examples \n",
    "                    last column indicates class membership\n",
    "                    remaining columns reflect features/attributes of data\n",
    "                             \n",
    "        output:\n",
    "            dict containing: 1) 'index' : index of feature used for splittling on\n",
    "                             2) 'value': value of threshold split on\n",
    "                             3) 'branches': tuple of data arrays reflecting the optimal split into left and right branches\n",
    "                             \n",
    "    \"\"\"\n",
    "    \n",
    "    # estimating the total number of classes by looking for the total number of different unique values \n",
    "    # in the final column of the data set (which represents class labels)\n",
    "    class_values=np.unique(dataset[:,-1])\n",
    "    \n",
    "    # initalising optimal values prior to refinment\n",
    "    best_cost=sys.float_info.max # initialise to max float\n",
    "    best_value=sys.float_info.max # initialise to max float\n",
    "    best_index=dataset.shape[1]+1 # initialise as greater than total number of features\n",
    "    best_split=tuple() # the best_split variable should contain the output of test_split that corresponds to the optimal cost\n",
    "    #1.4.1 iterating over all features/attributes (columns of dataset)\n",
    "    for index in np.arange(None):\n",
    "\n",
    "        #Trialling splits defined by each row value for this attribute\n",
    "        for r_index,row in enumerate(dataset):\n",
    "            # 1.4.2. return branches corresponding to thresholding on feaure (index) and threshold value (for row r_index)\n",
    "            branches=None\n",
    "\n",
    "            cost=None # 1.4.3 estimate cost for this split\n",
    "            if None: # 1.4.4. if this cost is an improvement on previous costs then save the \n",
    "                best_cost=None # cost\n",
    "                best_split=None # branches\n",
    "                best_index=None # feature index\n",
    "                best_value=None # threshold value\n",
    "                print('Best cost={}; Best feature={}; Best row={}'.format(best_cost,index,r_index) )\n",
    "                \n",
    "    return {'index':best_index, 'value':best_value, 'branches':best_split}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our functions for splitting on a single node are complete, let's find the best split of our toy dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split = get_best_split(X)\n",
    "\n",
    "print('The optimal left branch is \\n {}'.format(split['branches'][0]))\n",
    "print('The optimal right branch is \\n {}'.format(split['branches'][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected result is\n",
    "\n",
    "<img src=\"imgs/optimal_split.png\" style=\"max-width:100%; width: 50%; max-width: none\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "A decision tree is a hierarchy of decision stumps:\n",
    "\n",
    "<img src=\"imgs/decisiontree.png\" style=\"max-width:100%; width: 50%; max-width: none\">\n",
    "\n",
    "The top node is the root node and the terminal nodes are the leaf nodes; in between we refer to the input node of each decision stump as the parent nodes, which splits data down left and right branches to two child nodes\n",
    "\n",
    "As before, for decision stumps, nodes reflect questions we ask of the data ‚Äì e.g. threshold we choose (or, in the case of regression, constant functions we fit). Typically, fit on a single features. Edges then reflect the answers to that question ‚Äì binary choices ‚Äì as for classification stumps ‚Äìif a feature values Is less than a threshold it takes the left branch, if it‚Äôs more it takes the right branch\n",
    "\n",
    "This simplicity confers certain advantages:\n",
    "- it returns learning models which are easy to interpret \n",
    "- Requires little data preparation (no normalization)\n",
    "- Is able to handle both numerical and categorical data. \n",
    "- Is able to handle multi-output (multi-class) problems.\n",
    "\n",
    "And very importantly, the general approach can be applied for classification, where at each node an axis aligned classifier (threshold) is fit to optimally separate the data which reaches that node. For example, in the below (as explained in the video lecture). The algorithm splits first on feature $x_1$ to largely separate pink, red and yellow crosses from the light blue and dark green crosses (note crosses are chan ged to different shapes in video lectures to improve differentiation for those with colour blindness).\n",
    "\n",
    "The second decision stumps splits on $x_2$, and the third on $x_1$ again, all so as to partition the featurespace up into blocks which optimally separate the different classes. In this way, hopefully you can see it's able to learn a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('RFclassifier.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "# HTML(data='''  <video   alt=\"test\" controls>\n",
    "#                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "#              </video>'''.format(encoded.decode('ascii')))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"1000\" height=\"500\" controls>\n",
    "  <source src=\"RFclassifier.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, the approach is similar: a series of thresholds are made on the x-axis, and for each split a function is fit to minimise the error between true and predicted y values. In this case (and as standard for scikit learn implementations) the function fits a constant prediction at each split i.e. y = 0.6 and -1.1 for the left and right branches of the first split. As more and more splits are made the tree is able to estimate a closer and closer fit to this sinusoidal function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"1000\" height=\"500\" controls>\n",
    "  <source src=\"RFregression.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will build from our decision stump classiifer to create a full decision tree classifier learning algorithm. This section is optional but incorporates the ideas of learning a nested dictionary which stores the parameters of the weak learning rules for each stump in the tree:\n",
    "\n",
    "## (optional) Exercise 2: building and testing a complete decision tree\n",
    "\n",
    "<a id='Ex2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are able to evaluate the best split on a single node of a tree we can then start to think about building nodes together in order to refine prediction of class labels from our data. \n",
    "\n",
    "To complete this we need three more things:\n",
    "\n",
    "1. A function that assigns each leaf (terminal) node a label (associated with the most common label of training points reaching that node)\n",
    "2. A recursive function that decides (based on the configuration of data reaching each node) on whether to continue splitting the data or to assign that node a terminal node\n",
    "3. A function that recursively splits data (according to pt 2) in order to build a  tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1 labelling leaf nodes\n",
    "\n",
    "The goal of our tree is to make a prediction of class labels for unseen data. For classification problems, this means that each terminal node must have an assigned class. We do this by picking the most popular label from the training data that reach that class.\n",
    "\n",
    "1. from ```outcomes``` (class labels of the training set) estimate ```counts```: the total number of instances of each class (***HINT*** see numpy documentation for ```np.bincount```)\n",
    "    - bincount expects an integer array - os you must cast as type in using the .astype() attribute of numpy i.e. outcomes.astype(int)\n",
    "    \n",
    "2. finally return ```most_common_class```: the class with the biggest contribution to ```counts``` (***HINT***  np.argmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    \n",
    "    \"\"\"\n",
    "        Assigns a label according to the most common class label of the data\n",
    "        input:\n",
    "            group = array (n_samples,n_features+1) \n",
    "                    rows are examples \n",
    "                    last column indicates class membership\n",
    "                    remaining columns reflect features/attributes of data\n",
    "                             \n",
    "        output:\n",
    "            class label for this terminal node\n",
    "    \"\"\"\n",
    "    \n",
    "    # set outcomes equal to the final column of the input array  \n",
    "    # - as this indicates the labels of the training data \n",
    "    outcomes = group[:,-1]\n",
    "    counts = None\n",
    "    most_common_class=None\n",
    "    return most_common_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test on the split received by the left branch of the above example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_label=to_terminal(split['branches'][0])\n",
    "\n",
    "print('The label assigned to the left branch is:', left_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2. Recursively learning the full tree\n",
    "\n",
    "The next step is to generate a function (```run_split```) that will recursively split the data until a termination criterion is met. Termination criteria include:\n",
    "- the case where all examples are assigned to a single branch (and thus cannot be further subdivided)\n",
    "- the case where the node has reach a predefined ```max_depth``` for the tree\n",
    "- the case where the total number of examples reaching the node as reached or exceeded a pre-defined ``min_size```\n",
    "\n",
    "The input to the below function is a dictionary (here ```node``` ) representing the results output from ***Ex1.4*** containing the keys: ```index``` (feature that the node is split on), ```value``` (threshold on which the data is split) and ```branches``` (the tuple of data arrays reflecting the resulting split).\n",
    "\n",
    "The function starts by first extracting the split data from the node dictionary (key='```branches```') and then deleting this information from the node, such that the node can then be cleanly updated as a terminal node or split again.\n",
    "\n",
    "Then the code checks serially through all possible outcomes for the node:\n",
    "1. In the case that either the left or right branch of the split is empty, then ignore these branches and set this node as a terminal node; estimate the label for this prediction using ```to_terminal``` (STEP 1, above)\n",
    "2. In the case that the node is at the ```max_depth``` allowed for this tree then assign as terminal and estimate the label for this prediction using ```to_terminal``` (again STEP 1, above)\n",
    "3. In the case that the number of examples reaching the node is equal to or less than the ```min_size``` then, again, assign as terminal estimate the label for this prediction using ```to_terminal``` from STEP 1 above\n",
    "4. Finally, assuming that we instead have nodes that support further splits, for each ```left``` and ```right``` branches in turn, estimate ```get_best_split``` and then run the recursion (call ```run_split``` again)\n",
    "\n",
    "Note, a recursive function is a function which calls itself.\n",
    "\n",
    "Please familiarise yourself with the code of this function, to be sure that you understand exactly what each line of code is doing. Identify which lines correspond to each of points 1-4 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              \n",
    "def run_split(node, max_depth, min_size, depth):\n",
    "     \n",
    "    \"\"\"\n",
    "        Recursively splits nodes until termination criterion is met\n",
    "        input:\n",
    "            node = dict containing: 1) 'index' : index of feature used for splittling on\n",
    "                             2) 'value': value of threshold split on\n",
    "                             3) 'branches': tuple of data arrays reflecting the optimal split into left and right branches\n",
    "            max_depth: int determining max allowable depth for the tree\n",
    "            min_size : int determining minimum number of examples allowed for any branch\n",
    "            depth: current depth of tree              \n",
    "            \n",
    "            \n",
    "        Output:\n",
    "            node: is returned by value and returns a recursion of dicts representing the structure of the whole tree\n",
    "    \"\"\"\n",
    "    left, right = node['branches']\n",
    "    del(node['branches'])\n",
    "    # check for whether all data has been assigned to one branch; if so assign both branches the same label\n",
    "    if left.shape[0]==0 :\n",
    "        node['left'] = node['right'] = to_terminal(right)       \n",
    "        return\n",
    "    if right.shape[0]==0 :\n",
    "        node['left'] = node['right'] = to_terminal(left)       \n",
    "        return\n",
    "    # check for max depth; if exceeded then estimate labels for both branches\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "        # in first instance check whether the number of examples reaching the left node are less than the allowed limit\n",
    "        # if so assign as a terminal node, if not then split again\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_best_split(left)\n",
    "        run_split(node['left'], max_depth, min_size, depth+1)\n",
    "    \n",
    "    # process right child as for left\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_best_split(right)\n",
    "        run_split(node['right'], max_depth, min_size, depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3 Build the tree\n",
    "\n",
    "Finally, pool everything together through a top level ```build_tree``` function. Replace each ```None``` in this function with the correct code for complete construction of a tree through:\n",
    "\n",
    "1. create a root node split by calling ```get_best_split``` ([STEP 4](#step4)) on the full training set (line 11)\n",
    "2. recursively building the rest of the tree by calling ```run_split``` (line 13) \n",
    "\n",
    "These  completely specify the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(train, max_depth, min_size):\n",
    "    \"\"\"\n",
    "    Builds and returns final decision tree\n",
    "    \n",
    "    input:\n",
    "        train : training data array (n_samples,n_features)\n",
    "        max_depth: user defined max tree depth (int)\n",
    "        min_size: user defined minimum number of examples per tree tree depth (int)\n",
    "    \"\"\"\n",
    "    # create a root node split by calling get_best_split on the full training set\n",
    "    root = None\n",
    "    # now build the tree using run_split\n",
    "    run_split(None)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code on our full data set. Please edit the below call so that it trains on the full data set ```dataset``` with ```max_depth```= 3 and ```min_size```= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = build_tree(np.asarray(dataset), 3, 1)\n",
    "print('Decision Tree: \\n {}'.format(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can hopefully see, the left and right keys of the dictionary, also contain dictionaries at nodes where the data can be split further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tree we can make predictions. For this we require anpther recursive function that this time, checks whether the ```left``` and ```right``` branches at each new depth reflect new node dicts (in which case ```predict_row``` is called again recurvsively); otherwise, if a terminal node has been reached, the function returns a predicted class label:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(node, row):\n",
    "    \n",
    "    \"\"\"\n",
    "    Predict from a decision tree, by interogating node branches recursively\n",
    "    \n",
    "    input:\n",
    "        node = decision tree represented as dict containing: \n",
    "                1) 'index' : index of feature used for splittling on\n",
    "                2)  'value': value of threshold split on\n",
    "                3) 'branches': tuple of data arrays reflecting the optimal split into left and right branches\n",
    "        row: - single row of test data matrix    \n",
    "       \n",
    "    \"\"\"\n",
    "    if row[node['index']] < node['value']:\n",
    "         # if the result for the left branch returns another dictionary then repeat\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict_row(node['left'], row)\n",
    "        else:\n",
    "            # else if it's an integer you've reached a terminal node so return label\n",
    "            return node['left']\n",
    "    else:\n",
    "         # if the result for the right branch returns another dictionary then repeat\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict_row(node['right'], row)\n",
    "        else:\n",
    "            # else if it's an integer you've reached a terminal node so return label\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now taking a new data example ```[8.5,4.32,1]```, let us predict the correct label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata=np.asarray([8.5,4.32,1])\n",
    "\n",
    "prediction=predict_row(tree, testdata)\n",
    "print('Expected={}, Got={}'.format(testdata[-1], prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the below two functions will allow you to 1) predict for more than test example and 2) return a score - this will allow you to test the tree you have built against the scikit learn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree,testdata):\n",
    "    \"\"\"\n",
    "    Predict labels for a test data set\n",
    "    \n",
    "    input:\n",
    "        tree = decision tree represented as dict containing: \n",
    "                1) 'index' : index of feature used for splittling on\n",
    "                2)  'value': value of threshold split on\n",
    "                3) 'branches': tuple of data arrays reflecting the optimal split into left and right branches\n",
    "        test data: - an entire test data matrix    \n",
    "    \n",
    "    output: \n",
    "        predictions - predicted labels for all examples\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions=[]\n",
    "    for row in testdata:\n",
    "        node=deepcopy(tree)\n",
    "        predictions.append(predict_row(node, row))\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "def tree_score(testlabels, prediction):\n",
    "    \"\"\"\n",
    "    Return prediction score\n",
    "    \n",
    "    input:\n",
    "        testlabels: ground truth labels\n",
    "        prediction: predicted labels\n",
    "       \n",
    "    output:\n",
    "        score - accuracy of model\n",
    "    \"\"\"\n",
    "    \n",
    "    score=0\n",
    "    for i in np.arange(len(testlabels)):\n",
    "        if testlabels[i]==prediction[i]:\n",
    "            score+=1\n",
    "           \n",
    "    return score/len(testlabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (optional) Tree Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are prone to overfitting, as increasing the number of splits means that data becomes subdivided into leaves at ever finer levels of granularity - increasing the chance that the decision function becomes fit to noise in the data. This will reduce the generalisation performance, leading to lower test accuracies.\n",
    "\n",
    "For this reason all standard implementations of decision trees also offer pruning. This reduces overfitting by removing branches that contribute least to the prediction accuracy. Example methods for pruning include:\n",
    "\n",
    "- ***Reduced error pruning:*** starting from leaves, nodes are removed whilst prediction accuracy is unaffected\n",
    "-  ***Cost complexity pruning:*** This generates a series of trees¬†where¬† ùëá_0 ¬†is the initial tree and ùëá_ùëÄ ¬†represents the result of pruning everything away and leaving the root alone. The algorithm obtains these trees through iterative process. At each step:\n",
    "<br>\n",
    "    - Remove a subtree from tree ùëñ‚àí1, where the specific subtree to remove is chosen by minimizing: \n",
    "    \n",
    "    <br>\n",
    "    $$\\frac{ùëíùëüùëü(ùëá_ùëñ )‚àíùëíùëüùëü(ùëá_{i‚àí1}}{|ùëá_{ùëñ‚àí1} | ‚àí |ùëá_ùëñ|} $$\n",
    "    <br>\n",
    "    Then the best tree is selected from the list: ùëá_0 ‚Ä¶. ùëá_ùëÄ  so as to optimise training accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The basic algorithm for Regression Trees is the same for classification; however we must modify our cost and change how we make predictions from leaf nodes. \n",
    "\n",
    "For cost we can use Mean Squared Error (MSE), which minimises the L2 loss relative to the prediction. The prediction can be made several ways from the data reaching the node (see below figure): either through a constant function (that just fits the mean); a polynomial function (straight line or curve) fit more closely to the data. It is even possible to use a probabilistic model [Criminisi 2013] \n",
    "\n",
    "<img src=\"imgs/DT_regression_predictor_models.png\" style=\"max-width:100%; width: 70%; max-width: none\">\n",
    "\n",
    "In Scikit_Learn and most standard implementations simply the mean (case (a)) is used. MSE is then estimated as:\n",
    "\n",
    "$$ MSE= \\frac{1}{N} \\sum_k ^N (y_k-\\bar{y}_k)^2 $$\n",
    "\n",
    "Where, $y_i$ is the true label and $\\bar{y}_i$ is the mean of all data samples reaching that child node. The full cost of any split is again modelled as a weighted sum of costs for all child nodes:\n",
    "\n",
    "$$ I(S_j,\\theta_j) = \\sum_{i \\in {L,R}} \\frac{|S_j^i|}{|S_j|}MSE_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 -  Running Decision Trees with Scikit-Learn\n",
    "\n",
    "In this final exercise we will practice building Decision Tree classifiers using Scikit-Learn. Complete the below cell to\n",
    "\n",
    "1. instantiate a decision tree classifier model (line 29)\n",
    "2. fit the model to training data (line 30)\n",
    "3. Predict test labels (line 31)\n",
    "4. Return performance [score](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.score) on test examples (line 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # import the scikit-Learn Decision Tree module\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# CREATE a Random Data set using the sklearn Make Moons dataset\n",
    "\n",
    "DATA, LABELS =make_moons(noise=0.3, random_state=0)\n",
    "\n",
    "# Plot the data \n",
    "\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "ax = plt.subplot(1,1, 1)\n",
    "\n",
    "ax.set_title(\"Input data\")\n",
    "ax.scatter(DATA[:, 0], DATA[:, 1], c=LABELS, cmap=cm_bright,\n",
    "           edgecolors='k')\n",
    "\n",
    "# randomly split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(DATA, LABELS, test_size=.4, random_state=42)\n",
    "\n",
    "# **to DO** implement scikit learn decision tree classifier on this data\n",
    "#**** complete the above steps for the scikit learn classifier ******\n",
    "model = None\n",
    "None # fit model here\n",
    "pred=None #¬†predict test labels here\n",
    "score = None # output accuracy score\n",
    "\n",
    "print(\"Scikit-learn's decision tree Score\", score)\n",
    "\n",
    "# OPTIONALLY plot your results\n",
    "# suggest plotting with  with different colours for each class \n",
    "#and different markers for test and train data in order to aid visualisation\n",
    "\n",
    "# just plot the dataset first\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharey=True, figsize=(5,10))\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "ax1.set_title(\"True labels\")\n",
    "ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "           edgecolors='k')\n",
    "\n",
    "ax2.set_title(\"Predicted labels\")\n",
    "ax2.scatter(X_test[:, 0], X_test[:, 1], c=pred, cmap=cm_bright,\n",
    "           edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 3.2 (optional) Compare performance of scikit-Learn against your Decision Tree classifer\n",
    "\n",
    "You may wish to compare the performance of scikit learn against the Decision Tree classifier you have built during the session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first combine X_train and y_train togeher (and X_test, y_test) to put data into form expected by our tree\n",
    "dataset=np.concatenate((X_train,y_train.reshape((y_train.shape[0],1))),axis=1)\n",
    "test_dataset=np.concatenate((X_test,y_test.reshape((y_test.shape[0],1))),axis=1)\n",
    "\n",
    "#3.2.1 train your tree - set max depth to 5 and min size to 1\n",
    "tree = None\n",
    "\n",
    "#3.2.2 Get a prediction from your test data\n",
    "prediction_DT1=None\n",
    "\n",
    "#3.2.3 Score the accuracy of your decision tree classifier\n",
    "score_DT1=None\n",
    "print('Our Decision Tree Score', score_DT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try constructing a regression tree from scratch; using the above classification tree as the basis but:\n",
    "1.  creating a new MSE cost, and \n",
    "2.  editing the prediction function accordingly); \n",
    "\n",
    "Try it out the following toy dataset (Taken from:\n",
    "http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py)\n",
    "\n",
    "Compare your result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note these examples are inspired by the following on-line tutorial https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Criminisi 2013]  Criminisi, Antonio, and Jamie Shotton, eds. Decision forests for computer vision and medical image analysis. Springer Science & Business Media,\n",
    "\n",
    "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
