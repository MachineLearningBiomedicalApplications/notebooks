{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Ensemble Learning: Bagging, Boosting, Random Forests and Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now that we have learnt about weak learners, and how to code decision trees from scratch, we will move on to discussing ensemble learning with specific focus on homogenous ensembles.\n",
    "\n",
    "Let's start by importing the modules we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "\n",
    "The goal of ensemble learning is to improve the accuracy and/or generalisability of machine learning (ML) approaches by combining predictions from several models.\n",
    "\n",
    "There are several approaches to ensemble learning, which vary in whether they combine predictions made from the same class of base learners (otherwise known as __homogenous ensembles__) or whether they merge predictions from multiple different types of base learner (e.g. __heterogenous ensembles__). Ensemble methods also vary in how they combine predictions; specifically choosing either to generate base learners __sequentially__ or in __parallel__.\n",
    "\n",
    "The choice of sequential or parallel learning impacts differently on how the ensemble reduces model error. Remember, from the first lecture, the error emerging from any model can be broken down into three components:\n",
    "\n",
    "$$ùê∏ùëüùëü(ùë•)=ùêµùëñùëéùë†^2+ùëâùëéùëüùëñùëéùëõùëêùëí+ùêºùëüùëüùëíùëëùë¢ùëêùëñùëèùëôùëíùê∏ùëüùëüùëúùëü$$\n",
    "\n",
    "<img src=\"imgs/biasvariance2.png\" style=\"max-width:100%; width: 60%; max-width: none\">\n",
    "\n",
    "Learning **sequential ensembles** enables methods to exploit the dependence between base learners. Each new learner upweights mislabeled examples; thus learning a complementary set of predictors that reduce the bias and boost the accuracy of the model.\n",
    "\n",
    "On the other hand, **parallel ensembles** combine predictions learnt from multiple models run independently, and thus average away the impact of isolated errors, reducing the variance of the prediction.\n",
    "\n",
    "In this course we will discuss the following specific examples of homogenous ensembles:\n",
    "\n",
    "- Voting\n",
    "- Bagging (Bootstrapp Aggregation)\n",
    "- Boosting\n",
    "- Random Forests\n",
    "\n",
    "Of these, Boosting represents sequential ensemble learning and Bagging, voting and forests reflect parallel learning (note Random Forests are a generalisation of Bagging). \n",
    "\n",
    "In the next section, we will present a simple intuitive example of why ensemble learning works, through simple vote based aggregation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Voting: A toy example*\n",
    "\n",
    "*Taken from https://mlwave.com/kaggle-ensembling-guide/; this pages supplies number of excellent examples of ensemble learning (particular heterogenous forms) based on how they have performed on Kaggle Challenges\n",
    "\n",
    "Suppose we have a test set of 10 samples. The ground truth is all positive (‚Äú1‚Äù): ```y={1,1,1,1,1,1,1,1,1,1}```\n",
    "\n",
    "Furthermore, suppose we have 3 binary classifiers: A,B,C, each with 70% accuracy e.g.\n",
    "\n",
    "- $\\hat{y}_A={1,1,0,1,1,1,1,1,0,0} $\n",
    "- $\\hat{y}_B={0,1,1,1,1,0,1,1,0,1} $\n",
    "- $\\hat{y}_C={1,1,1,0,0,1,1,0,1,1} $\n",
    "\n",
    "\n",
    "If we were to combine these through majority voting (assigning a final class for each prediction from the most common class labels of each classifier) we then obtain:\n",
    "\n",
    "- $\\hat{y}_{MV}={1,1,1,1,1,1,1,1,0,1} $\n",
    "\n",
    "This has 90% accuracy since, in all but one case, at least two of the classifiers gave the correct prediction.\n",
    "\n",
    "Using probability theory we can see that for any combination of predictions for which each our ($ùë¶_ùëñ$) are correct 70% of the time, we can increase our accuracy to 78% (on average). This can be seen through the combination of two probability rules:\n",
    "1. For independent events the joint probability is the product of the marginals $P(\\hat{y}_A,\\hat{y}_B,\\hat{y}_C)=P(\\hat{y}_A)P(\\hat{y}_B)P(\\hat{y}_C)$,\n",
    "2. For mutually exclusive events (events which cannot both occur at the same time) the probability of either event $\\hat{y}_A$ or event $\\hat{y}_B$ occurring is: $P(\\hat{y}_A \\cup \\hat{y}_B)=P(\\hat{y}_A)+P(\\hat{y}_B)$\n",
    "\n",
    "Thus for a majority vote with 3 members we can expect 4 outcomes:\n",
    "\n",
    "1. All three are correct: <br>\n",
    "     0.7 \\* 0.7 \\* 0.7 <br>\n",
    "   = 0.343\n",
    "   \n",
    "2. Two are correct (here we have 3 mutually exclusive events each with _joint_ probabiities - $P(\\hat{y}_A=1,\\hat{y}_B=1,\\hat{y}_C=1)$ or $P(\\hat{y}_A=1,\\hat{y}_B=0,\\hat{y}_C=1)$ or $P(\\hat{y}_A=0,\\hat{y}_B=1,\\hat{y}_C=1)$: <br>\n",
    "     0.7 \\* 0.7 \\* 0.3  + 0.7 \\* 0.3 \\* 0.7 + 0.3 \\* 0.7 \\* 0.7  <br>\n",
    "   = 0.441\n",
    "   \n",
    "3. Two are wrong (here we have 3 mutually exclusive possibilities): <br>\n",
    "     0.7 \\* 0.3 \\* 0.3  + 0.3 \\* 0.7 \\* 0.3 + 0.3 \\* 0.3 \\* 0.7  <br>\n",
    "   = 0.189\n",
    "   \n",
    "3. All are wrong: <br>\n",
    "     0.3 \\* 0.3 \\* 0.3   <br>\n",
    "   = 0.027\n",
    "\n",
    "We can see the taking a majority vote of the ensemble will result in a correct answer when at least 2 of the predictions are correct. Thus majority voting will correct the result ~ 44% of the time, will already be correct  38% of the time (when all 3 are correct) and thus (again through use of the OR rule) the ensemble will thus be correct an average of 0.441+0.343 = 0.784 % of the time.\n",
    "\n",
    "Further if we were to repeat this experiment with more ensembles (**hint** try this) we would see that the prediction result increases. For example a voting ensemble of 5 classifiers with 70% accuracy would be correct ~83% of the time. \n",
    "\n",
    "However, this is only true, if we select only uncorrelated ensembles. If we instead train highly correlated predictive models then this would have limited or no impact:\n",
    "\n",
    "- $\\hat{y}_A={1,1,1,0,1,1,1,1,0,0} $ (70% accurate)\n",
    "- $\\hat{y}_B={1,1,1,0,1,1,1,1,0,0} $ (70% accurate)\n",
    "- $\\hat{y}_C={1,1,1,0,1,1,1,1,0,0} $ (70% accurate)\n",
    "\n",
    "\n",
    "A majority result generates $\\hat{y}_{mv}={1,1,1,0,1,1,1,1,0,0}$ which is still 70% accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# 2. Homogenous Ensembles\n",
    "\n",
    "## 2.1 Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Bagging is a method for generating homogenous ensembles in parallel. In simple terms, it repeatedly trains on slightly different training sets and then aggregates the results by, for example, majority voting or averaging. The reason why this works is that through averaging, bagging reduces the variance of the final prediction model, where it can be shown that, For $T$ independent samples (here, predictions from trained models):\n",
    "\n",
    "$$ var(\\bar{y}) = \\frac{var(y)}{T} $$\n",
    "\n",
    "Where, $\\bar{y}$ represents the mean of some quantity  $y$. \n",
    "\n",
    "Thus averaging appreciably reduces the variance of the model. But how can we get $T$ independent samples from our data if we have only collected one dataset?\n",
    "\n",
    "### 2.1.1 Bootstrapping \n",
    "\n",
    "We can do this using ***Bootstrapping***. This refers to generating new datasets by repeatedly sampling from the available data with replacement.  Conventionally, the size of the bootstrapped sample is equal to that of the original sample. We will now look at generating our own boostrap function to sample rows from our data matrices by taking advantage of the random sampling functions supplied with numpy (https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Exercise 1: Generating bootstrapped samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Complete the function ``` bootstrap_sample ``` to estimate a bootstrap sample from a given dataset of size (n_samples,n_features) using the numpy function  ```np.random.choice(a, size=None, replace=True, p=None)```, where ```a``` represents the total number of available examples (see [documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) for more details)\n",
    "\n",
    "**To do** \n",
    "\n",
    "1. Return an array of ```indices``` to sample rows from ```dataset``` by creating a sample from ```np.random.choice```\n",
    "    - **importantly bootstrapping expects sampling with replacement** and ```samples``` should have same number of rows as the original ```dataset```\n",
    "2. Use this to create a new data matrix ```samples``` which slices rows from the matrix ```dataset``` corresponding to each element of ```indices``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true
   },
   "source": [
    " <details>\n",
    "  <summary>Click me for hints </summary>\n",
    "\n",
    " - When the first argument of ```np.random.choice(a, ...)``` is an integer, it is treated as if you had supplied ```np.arange(a)``` (an array representing all indices from 0 to ```a```); this can be used to represent the ids of all rows in the original dataset.\n",
    " - We want an array of length equal to the number of rows of the current data array (set using 'size' argument)\n",
    " - we want to sample with replacement (by setting replace=True)  \n",
    "\n",
    "By way of example, let us assume we choose a=5, size = 6 and  replace=True\n",
    "Then np.random.choice(5, size=6, replace=true) might return the array  [4, 0, 1, 4, 2, 1]\n",
    "\n",
    "    <details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def bootstrap_sample(dataset,random_state=42):\n",
    "    ''' Create a random subsample from the dataset with replacement\n",
    "        input:\n",
    "            dataset: (n_samples,n_features) data array\n",
    "            random_state: fixes random seed\n",
    "        output:\n",
    "            samples: array of data bootstrapped from dataset\n",
    "    '''\n",
    "    n_sample = dataset.shape[0] \n",
    "    indices=None\n",
    "    \n",
    "    samples=dataset[indices]\n",
    "        \n",
    "    return np.asarray(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "2. Now test this function on sampling from the below dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dataset = np.asarray([[2.771244718,1.784783929,0],\n",
    "           [1.728571309,1.169761413,0],\n",
    "           [3.678319846,2.81281357,0],\n",
    "           [3.961043357,2.61995032,0],\n",
    "           [2.999208922,2.209014212,0],\n",
    "           [7.497545867,3.162953546,1],\n",
    "           [9.00220326,3.339047188,1],\n",
    "           [7.444542326,0.476683375,1],\n",
    "           [10.12493903,3.234550982,1],\n",
    "           [6.642287351,3.319983761,1]])\n",
    "\n",
    "test_sample=bootstrap_sample(dataset)\n",
    "print('The original data set \\n {}'.format(dataset))   \n",
    "print('Taking one bootstrapped sample returns \\n {}'.format(test_sample)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.2 Why Bootstrapping works..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "For large enough samples it can be shown that Bootstrap sampling will closely approximate any sampling distribution estimated from the full population. \n",
    "\n",
    "Take this example of estimating the mean from some population (simulated from random data as the variable ```DATA_population```). We can sample from this population many times using ```np.random.choice(..)``` to simulate the sample distribution of the population mean ```population_means```. \n",
    "\n",
    "Now, assume this full sample of the population is unavailable to us (as in most real world experiments). In that case we would generally try to approximate the population by taking a single sample; let's call this the ```DATA_sample```. In bootstrapping what we do is repeatedly sample from ```DATA_sample``` _with replacement_ to get $m$ different bootstrapped samples  of the mean (```sample_means```). What we can see from the below experiment, is that if our sample sizes are large enough, then the bootstrapped sample distribution comes very close to the population sample distribution. Observe how the histrograms representing each distribution overlap much more considerably for larger sample sizes\n",
    "\n",
    "<a id='bootsrapping'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# ######### TRIALING BOOTSTRAP SAMPLING FROM A  VECTOR #################\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# simulate the data from a population \n",
    "DATA_population=np.random.randint(0,20,100000)\n",
    "print('The mean of the population is {} '.format(np.mean(DATA_population)))   \n",
    "\n",
    "n_samples=100 # fix number of samples estimated to 100\n",
    "\n",
    "# comparing bootstrap and population sample distributions for different \n",
    "#initial sample_sizes\n",
    "for sample_size in [1, 100, 1000,5000,10000]:\n",
    "    # sample one example of size sample_size from our data population\n",
    "    DATA_sample=np.random.choice(DATA_population, sample_size) \n",
    "    sample_means = []; population_means = []\n",
    "    f=plt.figure()\n",
    "    for i in range(n_samples):\n",
    "        # repeatedly sample with replacement from our data population\n",
    "        population_sample = np.random.choice(DATA_population, sample_size) \n",
    "        # create a bootstrapped of our original sample\n",
    "        sample = bootstrap_sample(DATA_sample)\n",
    "        #compare mean of bootsrapped sample\n",
    "        sample_means.append(np.mean(sample))\n",
    "        # against mean of the sampling distribution\n",
    "        population_means.append(np.mean(population_sample))\n",
    "    # plot distributions on overlapping histograms\n",
    "    plt.hist(sample_means, 10, alpha=0.5, label='bootstrap')\n",
    "    plt.hist(population_means, 10, alpha=0.5, label='population')\n",
    "    if sample_size > 1:\n",
    "        # plot all but first with same x-range to allow fair comparison\n",
    "        plt.xlim((7.5,10.5))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Sample size={}, Estimated Population Mean: {},n Estimated bootstrapped mean: {} '\n",
    "              .format(sample_size, np.mean(population_means),np.mean(sample_means)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.3  Bagging: Aggregating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Returning to the concept of bagging. Here, Bootstrapping is used to generate $m$ different training sets. Bagging simply optimises a separate weak learner for each training set, runs test data through each weak learner and then aggregates the result, or step by step: \n",
    "\n",
    "1. Take original dataset $X$ with $N$ training examples\n",
    "2. Create _M_ copies  $\\{\\tilde{X}_ùëö \\}_{ùëö=1} ^{ùëÄ}$  by sampling _with replacement_\n",
    "    -  Each $\\tilde{X}_ùëö$ is different since some examples will get repeated and some will not be sampled at all\n",
    "3. Train a separate weak learner on each Bootstrap sample \n",
    "4. Test data on each weak learner\n",
    "5. Aggregate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### How to Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The method of aggregation changes depending on whether you are running a classification or regression task. For classification, the final prediction is typically a majority votes across all predictions, for each weak learner (in this case we assume these are _T_ decision trees):\n",
    "\n",
    "$$ \\bar{f}(\\mathbf{X})=mode \\{f_1(\\mathbf{X}), f_2(\\mathbf{X}).... f_T(\\mathbf{X}))\\} $$\n",
    "\n",
    "Whereas for regression the final prediction is an average over all tree predictions:\n",
    "\n",
    "$$ \\bar{f}(\\mathbf{x})=\\sum_{i=1}^T f_i(\\mathbf{x}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.4 Building a Bagging Classifer\n",
    "\n",
    "### Exercise 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now we have everything we need to construct a bagging classifer. As the weak learner we are going to use our decision tree from the last lecture. Let's import the decision tree model that we created last week (available in your Keats downloads as the file DecisionTree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import DecisionTree as DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now, all we need is two more functions:\n",
    "\n",
    "1. To create an ensemble of trees, by generating $M$ bootstrapped training data examples and using these to create $M$ different decision trees.\n",
    "2. To make a prediction by passing our test data down each tree in the ensemble, saving the predictions and then aggregrating them.\n",
    "\n",
    "Starting with \n",
    "\n",
    "1. Complete ```create_bagged_ensemble``` by creating a loop that\n",
    "    - creates a bootstrap sample of the data (using ```bootstrap_sample``` from above)\n",
    "    - builds a new decision tree from this training sample (***hint*** by calling function ```build_tree ``` from our imported ```DecisionTree``` module)\n",
    "    - adds this tree to the list ```bagged_ensemble```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def create_bagged_ensemble(data,max_depth, min_size, n_trees,random_state=42):\n",
    "    \n",
    "    ''' Create a bagged ensemble of decision trees\n",
    "    input:\n",
    "        data: (n_samples,n_features) data array\n",
    "        max_depth: max depth of trees\n",
    "        min_size: minimum number of samples allowed in tree leaf nodes\n",
    "        n_trees: total number of trees in the ensemble\n",
    "        random_state: fixes random seed\n",
    "    output:\n",
    "        bagged_ensemble: list of decision trees that make up the bagged ensemble\n",
    "    '''\n",
    "    bagged_ensemble=[]\n",
    "    \n",
    "    # complete this for loop including range (replace None) and 3 lines to complete above instructions\n",
    "\n",
    "    for i in range(n_trees):\n",
    "        #2.1.1 create bootstrapp sample\n",
    "        sample =None\n",
    "        # 2.1.2 build tree\n",
    "        tree = None\n",
    "        # 2.1.3 add tree to list bagged example\n",
    "        None\n",
    "    \n",
    "    return bagged_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Training on our toy data set. We create 100 bootstrapped trees, setting ```max_depth```= 3, and  ```min_size```= 1,   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "trees=create_bagged_ensemble(dataset,3, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "2.  Now complete function ```bagging_predict``` to aggregate predictions made for each test example \n",
    "\n",
    "Here the prediction are made for you, looping over all examples (```row in testdata```) and trees (```tree in trees```) Don't forget that now rather than returning a single prediction for each test example we now must estimate a prediction for each tree in the ensemble. Once obtained these must be aggregated using majority voting (as this is a classification task)\n",
    "\n",
    "**Ex 2.2 To do** Complete the ```bagging_predict``` function to aggregrate the classifier predictions:\n",
    "1. return a list containing the total number of predictions for each class (for each test example)\n",
    "2. For classification bagging aggregates using majority voting; return the ```row_label``` as the most common class using ```count```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "<details>\n",
    "  <summary> Click me for hints </summary>\n",
    "\n",
    "1. Estimate the total number of counts of each class using  (**hint** consider using np.bincount)\n",
    "2. From this return the most popular class using np.argmax\n",
    "    \n",
    "<details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def bagging_predict(trees, testdata):\n",
    "    \n",
    "    predictions=[]\n",
    "    # loop over all test examples (rows of the data matrix)\n",
    "    for row in testdata:\n",
    "        row_predictions=[]\n",
    "        # loop over all trees\n",
    "        for  tree in trees: \n",
    "            # get a prediction for that row (example) and that trees\n",
    "            # using function predict_row (line 241) of Decision Tree\n",
    "            tree_prediction=DT.predict_row(tree, row)\n",
    "            # append the prediction to the list of presdications for that example\n",
    "            row_predictions.append(tree_prediction)\n",
    "        \n",
    "        # 2.2.1 return the total number of predictions for each class\n",
    "        count=None\n",
    "        # 2.2.2 return predicted class\n",
    "        row_label=None\n",
    "        # append this to list of predictions for all test examples\n",
    "        predictions.append( row_label)\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Testing on a single example : ```[8.5,4.32,1] ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "testdata=np.asarray([8.5,4.32,1]).reshape((1,3))\n",
    "predictions=bagging_predict(trees, testdata)\n",
    "\n",
    "print('The Bagged prediction for this training row is {},\\\n",
    "      true label = {}'.format(predictions[0],testdata[0,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.5 Ensemble vs weak learners\n",
    "\n",
    "### Exercise 3 - Comparing our Bagged Predictor against our Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now that we have our own Bagging and Decision Tree classifiers we can compare them for a single classfication task. For this we use the ```make_moons``` test data set from Scikit-Learn. **Run these cells of code to test the performance of your DecisionTree and Bagging models**\n",
    "\n",
    "First let us import the data set and preprocessing functions from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "np.random.seed(1) # initialising random seed to ensure results are consistent each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now let us prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# create data set using scikit learn's make_moons data set\n",
    "DATA, LABELS =make_moons(noise=0.3, random_state=0)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "X = StandardScaler().fit_transform(DATA)\n",
    "\n",
    "# randomly split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, LABELS, test_size=.4, random_state=42)\n",
    "\n",
    "#fix variables\n",
    "max_depth = 5 # max tree depth\n",
    "min_size = 1 # min samples per leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Plotting ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(5, 5))\n",
    "cm = plt.cm.binary\n",
    "#cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "ax = plt.subplot(1,1, 1)\n",
    "\n",
    "ax.set_title(\"Input data\")\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='binary',\n",
    "           edgecolors='k', label='Train data')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='binary',\n",
    "           edgecolors='k', marker='^', label='Test data')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Here, training points are plotted as filled circles and test points are plotted as crosses. Each class is given by a different (red/blue) colour\n",
    "\n",
    "**To Do** Apply your methods for Decision Trees and Bagging. First reformatting the data such that the labels are concatenated to the last column of the data matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "dataset=np.concatenate((X_train,y_train.reshape((y_train.shape[0],1))),axis=1)\n",
    "test_dataset=np.concatenate((X_test,y_test.reshape((y_test.shape[0],1))),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now run training and prediction from the Decision Tree Model (created in the last lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tree = DT.build_tree(dataset,max_depth,min_size)\n",
    "prediction_DT1=DT.predict(tree, test_dataset)\n",
    "score_DT1=DT.score(y_test,prediction_DT1)\n",
    "print('Our Decision Tree Score', score_DT1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the true and predicted values for the test set, side by side (with colour indicating class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "ax.set_title(\"True values\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='binary',\n",
    "           edgecolors='k', marker='^', label='True values')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "cm = plt.cm.RdBu\n",
    "#ax = plt.subplot(1,1, 1)\n",
    "\n",
    "ax.set_title(\"Predicted values\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=prediction_DT1, cmap='binary',\n",
    "           edgecolors='k', marker='^', label='predicted values')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now run training and prediction for you new Bagging Classifier (for different numbers of trees):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "for n_trees in [1, 10, 50]: \n",
    "\n",
    "    tree_ensemble=create_bagged_ensemble(dataset,max_depth, min_size, n_trees)\n",
    "    prediction_BG1=bagging_predict(tree_ensemble, test_dataset)\n",
    "    score_BG1=DT.score(y_test,prediction_BG1)\n",
    "    print('Our Bagging Score', score_BG1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again plotting for the final bagged result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "ax.set_title(\"True values\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='binary',\n",
    "           edgecolors='k', marker='^', label='True Values')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "cm = plt.cm.RdBu\n",
    "#ax = plt.subplot(1,1, 1)\n",
    "\n",
    "ax.set_title(\"Bagged Predicted values\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=prediction_BG1, cmap='binary',\n",
    "           edgecolors='k', marker='^', label='Bagged predictions')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Clearly, the bagging classifier improves the prediction result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Exercise 4 - Comparing against Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Scikit learns functions for Decision Tree and Bagging classifiers can be found http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html and  http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html.\n",
    "\n",
    "Importing these and complete the code below to compare against the results from exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# 4.1 instaniate a scikit learn decision tree classifier\n",
    "tree_model=None\n",
    "# 4.2. fit the model\n",
    "None\n",
    "# 4.3 return the accuracy on the test set (i.e. score)\n",
    "score_DT2 = None\n",
    "# 4.4 return prediction on the test set\n",
    "prediction_DT2=None\n",
    "\n",
    "print('Sklearn Decision Tree Score', score_DT2)\n",
    "# Bagging 4.5 instaniate a scikit learn bagging classifier\n",
    "bagging_model=None\n",
    "# 4.6. fit the model\n",
    "None\n",
    "# 4.7 return the accuracy on the test set (i.e. score)\n",
    "score_BG2 = None\n",
    "print('Scikit-Learn Bagging score {} using {} Trees'.format(score_BG1,n_trees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "It is not surprising the the Scikit Learn decision tree module outperforms ours as it refines its trees through pruning to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.6 Estimating Out-Of-Bag estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "To evaluate the test error of a bagging estimate we could use cross-validation. However, with each training set we automatically get a left out set. For large enough sample sizes ~37 % of training points are left out. We can use these as a validation set, in order to estimate what is known as the ***Out-of-Bag (OOB) Error***. This can be achieved using the following steps:\n",
    "\n",
    "- For each sample $x_i$, \n",
    "\n",
    "  - Find the prediction $\\hat{y}_i^b$ for all bootstrap samples b which do not contain $x_i$\n",
    "  - Average/combine these predictions to obtain $\\hat{y}_i^{oob}$ (aggregated score for each training example)\n",
    "  - Combine to return accuracy/prediction score over all out-of-bag samples\n",
    "  - (for classification) estimate error as $1- oob_{score}$\n",
    "  \n",
    "The scikit-learn Bagging function has inbuilt functionality to calculate the OOB error. This can be utilised by calling the ```BaggingClassifier ``` with the argument ```oob_score```\n",
    "\n",
    "Plotting the OOB error for different numbers of trees can be observed through the following Scikit-Learn toy dataset problem (adapted from http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X, y = make_classification(n_samples=1000, n_features=25,\n",
    "                           n_clusters_per_class=1, n_informative=15,\n",
    "                           random_state=RANDOM_STATE)\n",
    "\n",
    "\n",
    "estimator_numbers=np.linspace(50,400,50).astype(int)\n",
    "error_rate=[]\n",
    "for n_trees in estimator_numbers: \n",
    "    clf=BaggingClassifier(DecisionTreeClassifier(),n_estimators=n_trees, oob_score=True)\n",
    "    clf.fit(X, y)\n",
    "    oob_error = 1 - clf.oob_score_\n",
    "    error_rate.append(oob_error)\n",
    "    \n",
    "plt.plot(estimator_numbers,error_rate)\n",
    "plt.title('OOB error for Bagged Classifier')\n",
    "plt.xlabel('number of trees')\n",
    "plt.ylabel('OOB error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.1.7  When not to use Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Bagging is useful for datasets that are noisy and have high variance. In other words if the predictions generated across bagged trees (or any other weak learner) vary significantly from one another then Bagging can significantly help to reduce the variance. \n",
    "\n",
    "If, on the other hand, the varying the training dataset has little impact on the final predictions then Bagging will not be beneficial. On the contrary, it can be found to degrade the performance of already stable classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.2. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Random Forests, first proposed by Adit and Geman {AG1997], and later refined by Breiman [Br2001], are an extension of bagging, which increase randomisation further by sampling from the parameter space ($T$) at each tree node. Thus, when optimising a split of the incomining data, a node must select, not from all features in the data set, but from a subset made available to each root node. A guiding rule of thumb is that each node might see only $d=\\sqrt(m)$ or $d=\\log_2 m$ of the total number of features $m$.\n",
    "\n",
    "In this way the assembled trees are further decorrelated from one another than they would be if standard decision trees were constructed and bagged. More specifically, training subsets will take more and more different paths through the decision trees, resulting in greater differences in the distribution of data across leaf nodes.  This has the effect of further decreasing the variance of the ensemble; increasing robustness to noisy training examples and/or features, whose influence gets spread out and diluted across the ensemble.\n",
    "\n",
    "Training and testing random forests therefore behaves very similarly to that of bagged ensembles of decision trees, with the exception that when optimising for the best split of the data at each node (e.g. see ```get_best_split``` from DecisionTree.py) the data must first be modified to ensure that only a subset of the full feature-space is passed to  the splitting function (e.g. ```test_split```). A summary of the training process is shown below:\n",
    "\n",
    "\n",
    "<img src=\"imgs/randomforest_train.png\" style=\"max-width:100%; width: 80%; max-width: none\">\n",
    "\n",
    "\n",
    "Testing then proceeds exactly as before:\n",
    "\n",
    "\n",
    "<img src=\"imgs/randomforest_test.png\" style=\"max-width:100%; width: 80%; max-width: none\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.2.1 Applying Random Forests using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The documentation for Scikit-Learn's Random Forest methods may be found http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html and http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "Important arguments include the option of tuning the ```max_depth``` and ```min_samples_leaf``` (min samples per leaf) for each tree. The number of features available for each node may be tuned through the ```max_features``` argument, the total number of trees in the ensemble is controlled using ```n_estimators``` and the argument for estimating the OOB error is ```oob_score ``` as before. As with all Scikit-Learn modules, to fix the random seed (and thus ensure you return the same results every time) set the ```random_seed```.\n",
    "\n",
    "An example of how to call the Scikit Learn RF classifier is shown below, appled to the ```make_moons``` data set, (imported and pre-processed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf=RandomForestClassifier(max_depth=max_depth, n_estimators=50, random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "score_RF = clf.score(X_test, y_test)\n",
    "\n",
    "print('Scikit-Learn Random Forest score on make moons data set', score_RF)\n",
    "print('Scikit-Learn Decision Tree score on make moons data set', score_DT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Exercise 5: Training Random Forests to Predict Gestational Age from Regional Brain Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Apply Scikit-Learn's Random Forest Regressor to the task of predicting gestational age (GA) from regional brain volumes. Complete the code below to:\n",
    "\n",
    "   1. implement the training and testing of the ```DecisionTreeRegressor()``` \n",
    "   2. implement the training and testing of the  ```RandomForestRegressor()``` \n",
    "   3. (optional) Return and plot feature importances see the [scikit-learn tutorial](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) for guidance\n",
    "   \n",
    "Data pre-processing has been performed for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "## from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load data\n",
    "\n",
    "DATAMAT=pd.read_csv('GA-structure-volumes-preterm.csv',header=None)\n",
    "\n",
    "# separate out data from labals\n",
    "\n",
    "DATA = DATAMAT.loc[:,1:] # volumes - we have 86 features and 164 samples\n",
    "LABELS = DATAMAT[0] # GA - 164 \n",
    "\n",
    "# split data into test and train\n",
    "X_train_GA, X_test_GA, y_train_GA, y_test_GA = train_test_split(DATA, LABELS, test_size=.4, random_state=42)\n",
    "\n",
    "# 5.1 get baseline prediction from decision tree (no param optimisation)\n",
    "# 5.1.1 create instance of DecisionTreeRegressor with default parameters (1 line)\n",
    "tree_model=None\n",
    "# 5.1.2 train\n",
    "None\n",
    "# 5.1.3 Test\n",
    "score_DT = None\n",
    "print('Decision Tree Score', score_DT)\n",
    "\n",
    "# 5.2 get baseline prediction from Random Forest (no param optimisation)\n",
    "# 5.2.1 create instance of RandomForestRegressor with default parameters \n",
    "forest_model=None\n",
    "# 5.2.2 train\n",
    "None\n",
    "# 5.2.3 test\n",
    "score_RF1 = None\n",
    "print('Random Forest initial Score', score_RF1)\n",
    "\n",
    "# 5.3. get feature importances\n",
    "importances = None\n",
    "std = None\n",
    "# get indices that sort the importance through sorting by _decreasing_ value\n",
    "# note to list an array (foo) in reverse order use foo[::-1] - https://stackoverflow.com/questions/509211/understanding-slice-notation\n",
    "indices = None\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(20):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the 20 most important features in the forest\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(20), importances[indices][0:20],\n",
    "        color=\"r\", yerr=std[indices][0:20], align=\"center\")\n",
    "plt.xticks(range(20), indices[0:20])\n",
    "plt.xlim([-1, 20])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### 2.2.2 Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "source": [
    "As indicated above, the Scikik-learn module allows for tuning of several key parameters of the model. Of these, perhaps the most important are:\n",
    "\n",
    "- Number of features\n",
    "- Max depth of trees\n",
    "- Number of estimators\n",
    "\n",
    "We will now use Scikit-Learn's ```GridSearchCV``` function to optimise the parameters of our Random Forest and improve performance on the above problem.\n",
    "\n",
    "For help on how to implement parameter optimisation using GridSearch, look at the scikit learn [documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Exercise 6 (optional):  Perform Parameter Optimisation for Random Forests using ```GridSearchCV```\n",
    "\n",
    "Implement parameter optimisation using ```GridSearchCV``` by:\n",
    "   1. Creating a ```dict``` called ```param_dist``` which specifies parameters for ```max_depth```, ```max_features``` and ```n_estimators ```\n",
    "   2. create an instance (```grid```) of  ```GridSearchCV``` with ```RandomForestRegressor()``` as the model and ```param_dist``` as the search grid (with cross validation: ```cv=5``` and ```iid=False```; then run on the training data\n",
    "   3. Apply the tuned parameters to the model and get a new test prediction\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "# 6.1 specify parameters and distributions to sample from in the form of a dict (3 lines)\n",
    "param_dict = None\n",
    "\n",
    "model=RandomForestRegressor(random_state=42)\n",
    "# 6.2 create an instance of GridSearchCV (1 line)\n",
    "grid = None\n",
    "# run gridsearch on the training data (1 line)  \n",
    "grid.fit(X_train_GA, y_train_GA)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print('Best classification score achieved using grid search:', grid.best_score_)\n",
    "print('The parameters resulting in the best score are depth: {},max_f {} and n_estimators {} '\\\n",
    "      .format(grid.best_estimator_.max_depth,grid.best_estimator_.max_features,\\\n",
    "              grid.best_estimator_.n_estimators))\n",
    "\n",
    "#6.3.1 create RF model using optimised parameters (1 lines)\n",
    "model=None\n",
    "\n",
    "#6.3.2 fit model to training data\n",
    "None\n",
    "#6.3.3 Test \n",
    "score_RF2 = None\n",
    "print('Random Forest refined test Score', score_RF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 (optional) Building a Random Forest Classifier from scratch\n",
    "\n",
    "In short this will require simple copying and pasting of the existing code (for Decision Trees and Bagging) followed by minor modification of the ```get_best_split``` function to ensure that only a subset of the available features are evaluated by ```test_split```.\n",
    "\n",
    "Give this a go, and then test on the ```make_moons``` data set, as demoed previously for the DecisionTree and Bagging modules, and as shown below for the Scikit-Learn methods.\n",
    "\n",
    "**To do** \n",
    "\n",
    "1. copy and paste the code for Bagging.py into a new script for a random forest module. \n",
    "2. edit ```get_best_split``` as described above\n",
    "3. test on the scikit learn ```make_moons``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do - test your random forest on make moons here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 2.3 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a method for sequential ensemble learning that works by building simple prediction models, one after the other, and reweighting training data each time to give added importance to training examples that were misclassified in the previous round. In this way, each new predictor is encouraged to work harder on border cases, incrementally reducing the bias of the model. \n",
    "\n",
    "The basic principle of boosting is shown below for the given toy example:\n",
    "\n",
    "<img src=\"imgs/adaboosttoyexample.png\" style=\"max-width:100%; width: 40%; max-width: none\">\n",
    "\n",
    "Here, a simple classification problem is shown for two classes (positive=red cross and negative=blue minuses). A simple classifier is fit based on thresholding on a single feature (red rectangle). This classifies everything to the left hand side as positive. We see that 3 red crosses are therefore missclassified by this rule. These examples are then reweighted to give them greater importance in the next training round:\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/adaboosttoyexample_round2.png\" style=\"max-width:100%; width: 60%; max-width: none\">\n",
    "\n",
    "In this round, examples that were previously misclassified are upweighted (reflected by increase in size) and those that were correctly classified are downweighted (made smaller). A second model is fit, shown by the new red box. Now three negative examples are misclassified. These are upweighted in a third round: \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/adaboosttoyexample_round3.png\" style=\"max-width:100%; width: 60%; max-width: none\">\n",
    "\n",
    "This creates a third proposed weak-learner (or split of the data). The three weak rules are then subsequently combined together to generate one final prediction model:\n",
    "\n",
    "<img src=\"imgs/adaboosttoyexample_final.png\" style=\"max-width:100%; width: 50%; max-width: none\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several variants of boosting; however in this lecture we will only consider the first: Adaboost [Fr1997], addressing the following implementational choices:\n",
    "\n",
    "- What to use as weak learners?\n",
    "- How to weight the examples?\n",
    "- How many weak learners should be learnt?\n",
    "- How to combine weak learners at the end? \n",
    "\n",
    "In the original form of Adaboost the choice of weak learner is effectively a simple threshold on a single feature in the data, otherwise known as a decision stump (or decision tree with only once split):\n",
    "\n",
    "<img src=\"imgs/decisionstump.png\" style=\"max-width:100%; width: 20%; max-width: none\">\n",
    "\n",
    "However, it is important to note that modern implementations (including that of Scikit-Learn) open the possibility for parameterising Adaboost to use deeper trees (this option can be tuned during parameter optimisation).\n",
    "\n",
    "Examples are initially all given the same weight ($w_i=1/n$, for $n$ = the number of examples). Then these are down-weighted if correctly classified (or regressed precisely) and left unchanged if incorrectly predicted\n",
    "\n",
    "Each weak learner‚Äôs job is to find the best weak hypothesis $f_t (\\mathbf{X}, \\theta) : \\mathbb{R}^d \\times \\Gamma \\rightarrow {0,1}$  (for classification) suited to fitting to the current weighted distribution of examples $\\mathbf{p}^t$. Subsquently, the weighted error of the prediction is calculated ($\\epsilon_t$) and weights are redistributed according the performance of the given weak learner (through $\\beta$). \n",
    "\n",
    "### 2.3.2 Adaboost Classification\n",
    "\n",
    "For classification, the boosting algorithm is given by the following algorithm:\n",
    "\n",
    "<img src=\"imgs/adaboost_algorithm.png\" style=\"max-width:100%; width: 70%; max-width: none\">\n",
    "\n",
    "Specifically, correctly classified examples are downweighted by assigning weights $w_i^{t+1}$ equal to $w_i^{t}\\beta_t$ (whereas misclassified examples retain weight $w_i^{t}$). This applies as $|f_t(\\mathbf{x_i})-y_i|=0$ for correctly classified examples and $|f_t(\\mathbf{x_i})-y_i|=1$ for incorrectly classified examples.  The final hypothesis is then estimated from a weighted contribution of all weak learners as follows:\n",
    "\n",
    "$$ F(\\mathbf{x_i})= \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      1  \\textrm{    if   } \\sum_t^T \\ln(1/\\beta_t)f_t(\\mathbf{x_i}) \\geq \\sum_t^T \\ln(1/\\beta_t) \\\\\n",
    "      0  \\textrm{   otherwise }\n",
    "    \\end{array}\n",
    "  \\right.$$\n",
    "\n",
    "Thus, Adaboost reduces the overall model error by incrementally improving classification of border cases, so as to reduce bias, over $T$ rounds of classification. Boosting can also be seen to be robust to overfitting (thus reducing model variance). This is typically explained through the observation that even when training error has been reduced to zero, further refinements continue to be made on the estimation of marginal cases. In this way the confidence of the predictions made on the training data continues to increase, resulting in a better generalisation performance. See [Sch2013] for a more complete explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Adaboost Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original algorithm [Fr1997] was designed for classification, but [Dr1997] proposes a modification suited to regression.\n",
    "\n",
    "In this, the algorithm proceeds as above but regression specific loss functions are used to estimate prediction error (these can be linear, squared or exponential function [Dr1997]), and thus subsequently the next round of weights:\n",
    "\n",
    "<img src=\"imgs/adaboost_regression1.png\" style=\"max-width:100%; width: 60%; max-width: none\">\n",
    "\n",
    "The final prediction is then estimated from the weighted median of predictions from all $T$ weak learners (where weights are defined as $\\ln(1/\\beta_t)$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Applying Adaboost using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation for Scikit-Learn's Adaboost methods may be found http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html and http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor\n",
    "\n",
    "Important arguments include the choice of baselearner (by default this is a decision tree), the number of estimators fit, and (for regression only) the choice of loss function (linear, squared, exponential). Again, an argument ```random_state``` is available to fix the seed of the random number generator (to ensure consistency of results over multiple runs).\n",
    "\n",
    "An example of how to call the Scikit Learn Adaboost classifier is shown below, appled to the make_moons data set, (imported and pre-processed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# train Adaboost on make_moons training set defined above\n",
    "\n",
    "clf=AdaBoostClassifier(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "score_AB = clf.score(X_test, y_test)\n",
    "print('Scikit-Learn AB score', score_AB)\n",
    "\n",
    "#recalling previous results\n",
    "print('Scikit-Learn Random Forest score on make moons data set', score_RF)\n",
    "print('Scikit-Learn Decision Tree score on make moons data set', score_DT2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Training Adaboost to Predict Gestational Age from Regional Brain Volumes¬∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Scikit-Learn's Adaboost regressor to the task of predicting gestational age (GA) from regional brain volumes. \n",
    "\n",
    "Using the test training data split from above (```X_train_GA, X_test_GA, y_train_GA, y_test_GA```) complete the code below to:\n",
    "   1.  Trial Adaboost Regression using default parameters\n",
    "   2.  Try optimising parameters (such as min_samples_leaf and n_estimators) using GridSearchCV. Note, the entry for min_samples_leaf in the parameter dict has been completed for you to demonstrate how to tune the parameters of a base learner (in this case a decision tree)\n",
    "   3.  Apply the optimised parameters to train and test a new model. (Note given the parameters in param_dist I was not able to improve the test result - see if you can do better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# 8 get baseline prediction from Adaboost (no param optimisation)\n",
    "# 8.1 create instance of Adaboost with default parameters (1 line)\n",
    "clf=AdaBoostRegressor(random_state=42)\n",
    "# 8.1.1 Train on data X_train_GA, y_train_GA (1 line)\n",
    "clf.fit(X_train_GA, y_train_GA)\n",
    "# 8.1.2 Test\n",
    "score_AB1 = clf.score(X_test_GA, y_test_GA)\n",
    "print('Adaboost initial Score', score_AB1)\n",
    "\n",
    "\n",
    "# optimise parameters using GridSearchCV\n",
    "# specify parameters and distributions to sample from in the\n",
    "#form of a dict (2 lines)\n",
    "param_dist = {\"base_estimator__min_samples_leaf\": [1,2,3, 5, 10],\n",
    "              \"n_estimators\": [5,10,25,10, 50,100,150,200,500]\n",
    "              }\n",
    "\n",
    "model=AdaBoostRegressor(DecisionTreeRegressor())\n",
    "# 8.2 create an instance of GridSearchCV (1 line)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_dist,cv=5,scoring='neg_mean_squared_error')\n",
    "# 8.3 run gridsearch on the training data (1 line)  \n",
    "grid.fit(DATA, LABELS)\n",
    "\n",
    "print('Best regression score achieved using grid search:', grid.best_score_)\n",
    "print('The parameters resulting in the best score are min samples per leaf: {}, \\\n",
    "      and n_estimators {} '.format(\n",
    "    grid.best_estimator_.base_estimator_.min_samples_leaf,grid.best_estimator_.n_estimators))\n",
    "\n",
    "# 8.3.1 create RF model using optimised parameters (1 lines)\n",
    "model=AdaBoostRegressor(base_estimator=DecisionTreeRegressor(\n",
    "    min_samples_leaf=grid.best_estimator_.base_estimator_.min_samples_leaf),\n",
    "                            n_estimators=grid.best_estimator_.n_estimators,\n",
    "                            random_state=42)\n",
    "# 8.3.1 fit model to training data\n",
    "model.fit(X_train_GA, y_train_GA)\n",
    "# 8.3.1 Test \n",
    "score_AB2 = model.score(X_test_GA, y_test_GA)\n",
    "print('Adaboost refined test Score', score_AB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AG1997] Amit, Yali, and Donald Geman. \"Shape quantization and recognition with randomized trees.\" Neural computation 9.7 (1997): 1545-1588.\n",
    "\n",
    "[Br2001] Breiman, Leo. \"Random forests.\" Machine learning 45.1 (2001): 5-32.\n",
    "\n",
    "[Fr1997] Y. Freund, and R. Schapire, ‚ÄúA Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting‚Äù, 1997.\n",
    "\n",
    "[Sch2013] Schapire, Robert E. \"Explaining adaboost.\"¬†Empirical inference. Springer, Berlin, Heidelberg, 2013. 37-52.\n",
    "\n",
    "[Dr1997] Drucker. ‚ÄúImproving Regressors using Boosting Techniques‚Äù, 1997.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
