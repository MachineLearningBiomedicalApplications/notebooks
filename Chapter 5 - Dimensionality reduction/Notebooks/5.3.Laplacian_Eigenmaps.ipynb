{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Eigenmaps\n",
    "\n",
    "Based on 2019 notebook written by Dr James Clough\n",
    "\n",
    "Use of manifold learning is motivated, for many biomedical applications, by the fact that our datasets has very high dimensionality and this makes analysis challenging. At the same, time data will usually be generated by some natural process with fewer degress of freedom than the dimensionality of the data would suggest. Manifold learning techniques provide a framework for determining these underlying degrees of freedom in the data.\n",
    "\n",
    "To understand this better, let's look at a simple example in which linear methods like PCA fail to uncover the data's natural non-linear structure.\n",
    "\n",
    "Starting by importing all the modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D # for 3D plotting with matplotlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA   # sklearn has a nice PCA implementation\n",
    "from scipy.linalg import eigh           # eigendecomposition\n",
    "\n",
    "# display plots in the notebook\n",
    "%matplotlib inline    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - applying principal component analysis to the swiss roll problem\n",
    "\n",
    "First, we need to generate the swiss-roll dataset. We start with a set of  ùëÅ  points scattered in a 2D unit square, and will then map those points to lie on a spiral shape in 3D. Here, parameter num_rotations sets the number of full rotations of the swiss-roll spiral. Run the code below - if you like, you can try changing the variables here to generate different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spiral(M, num_rotations):\n",
    "    \"\"\" Take 2D manifold M and output 3D spiral made from curling up M in 3D space \"\"\"\n",
    "    N = M.shape[0]\n",
    "    r = np.exp(M[:,1] * num_rotations) * 0.5\n",
    "    theta = M[:,1] * (2 * np.pi) * num_rotations\n",
    "    X = np.zeros((N, 3))\n",
    "    X[:,0] = M[:,0] * 6\n",
    "    X[:,1] = r * np.cos(theta)\n",
    "    X[:,2] = r * np.sin(theta)\n",
    "    return X\n",
    "\n",
    "N = 2000                       # number of datapoints\n",
    "num_rotations = 1.2\n",
    "X_m = np.random.random((N, 2)) # random points scattered in [0,1]^2\n",
    "\n",
    "X = create_spiral(X_m, num_rotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have generated the swiss-roll dataset, we can plot it. Think of the original 2D data as the 'underlying degrees of freedom. Think of the 3D data as the high-dimensional data we have measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot the manifold curled up into a 3D swiss-roll\"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.scatter(X_m[:,0], X_m[:,1], c=X_m[:,1], marker='o')\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "_ = ax.scatter(X[:,0], X[:,1], X[:,2], c=X_m[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA projects the data onto the hyperplane minimising the approximation error, or, equivalently, maxmising the variance along the hyperplane.\n",
    "\n",
    "**What do you expect to happen if we use PCA to reduce the dimensionality of this swiss-roll dataset from 3 dimensions to 2?**\n",
    "\n",
    "**To do** fit sklearns' PCA model to the spiral dataset we just generated by replaceing the ```None```s in the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Try using PCA on this dataset and see if it can recover the underlying 2D structure \"\"\"\n",
    "\n",
    "# In scikit-learn, we first create a PCA object...\n",
    "# n_components refers to the number of principal components we're extracting\n",
    "# ie. the dimensionality of the space we are trying to find\n",
    "pca = None\n",
    "\n",
    "# Then apply in to our dataset\n",
    "X_pca = None\n",
    "\n",
    "# plotting the result of the PCA projection\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "_ = plt.scatter(X_pca[:,0], X_pca[:,1], c=X_m[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is going wrong here?**\n",
    "\n",
    "The exact shape of the data you see will vary depending on the random points you started with, but it is unlikely that you will recover something very much like our original 2D dataset, and instead the spiral structure we are trying to unfold will still be there.\n",
    "\n",
    "We have failed to uncover the 2D manifold structure because PCA can only project the data on to a hyperplane and it can't unfurl curved manifolds.\n",
    "\n",
    "Another way of thinking about this problem is to notice that we want to measure the distances between points as the distance along the manifold (ie. along the swiss-roll) but if the manifold is curved, that is not the same as the distance between the points in the high-dimensional space (ie. the distance in 3D space not confined to the surface of the swiss-roll)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning embeddings from the eigenvectors of graph Laplacian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try and solve this problem we need non-linear manifold learning.\n",
    "\n",
    "There are lots of non-linear manifold learning algorithms, but we'll be using one called Laplacian Eigenmaps, which is a simple but effective and commonly used algorithm.\n",
    "\n",
    "**Laplacian Eigenmaps for Dimensionality Reduction and Data Representation**, *Belkin and Niyogi, Neural Computation, 2003*\n",
    "\n",
    "The key idea is this that even if the manifold that the data lies on is curved, we can assume that it is locally flat. Specifically, we will trust Euclidean distances in the local neighbourhood of each point, but will ignore (remove) distances from all but our k-nearest neighbours in what follows.\n",
    "\n",
    "### Estimating a Nearest Neighbour Graph\n",
    "\n",
    "The first step in many manifold learning methods is to find the k-nearest neighbours of each point. \n",
    "\n",
    "This just means that we choose a number, $k$, and for each point in our data, find the $k$ points which are nearest to it. The result can be described mathematically as a ***graph***. \n",
    "\n",
    "Here, a graph is not the thing with an x-axis and a y-axis and a line - rather it is a mathematical object (studied in Graph Theory - https://en.wikipedia.org/wiki/Graph_theory) that consists of some points, or **vertices**, and some relations between them, or **edges**.\n",
    "\n",
    "In this case, the vertices in the graph are our datapoints, and there will be an edge between two points if one of them is in the k-nearest neighours of the other.\n",
    "\n",
    "<img src=\"imgs/scatter.png\">\n",
    "\n",
    "<img src=\"imgs/scatter_knn.png\">\n",
    "\n",
    "Let's write a function that finds the k-nearest neighbours of each point in our dataset.\n",
    "\n",
    "### Exercise 2 - Create a _symmetric_ k-Nearest neighbour graph\n",
    "\n",
    "Write a function, ***my_knn*** that takes in an array of size NxD of the datapoints, and an integer k, and returns an NxN array where the [i,j] element of the array is a 1 if j is a k-nearest-neighbour of i (excluding i itself), and 0 otherwise.\n",
    "\n",
    "Don't worry too much about doing this in a clever or fast way, just make sure your function gives you the correct answer.\n",
    "\n",
    "**To do** complete this kNN function to implement the steps of k-nearest neighbour implementation (up to but not including making it symmetric)\n",
    "\n",
    "**Steps**\n",
    "1. Estimate the squared distances between point i and all neighbours\n",
    "2. Find the indices of the nearest neighbours (2 lines) *Hint - numpy has a function called **argsort** that you might find useful*\n",
    "3. Input the nearest neighbours for the point of $i$ by filling in row $i$ of the matrix $\\mathbf{A}$\n",
    "\n",
    "**If you get stuck there are more hints that you can lock by viewing the hidden cell below (go to view->cell Toobar-> toggle ```Hide code``` and uncheck)**\n",
    "\n",
    "**Hidden cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Some extra hints for those who are stuck\n",
    "\n",
    "#1. For each loop you need to estimate the square distance between a point ```X[i]``` and all other points.\n",
    "#    - Python can do this using broadcasting to subtract vector ``X[i]``` from X\n",
    "#    - don't forget pythagoras c^2 = a^2 + b^2 - you need a sum of square over the features (columns) \n",
    "#¬†   - will return a scalar distance between each point (there are N of them)\n",
    "\n",
    "# 2 To obtain the indices of the closest neighbour you need two steps \n",
    "#     - sort by increasing distance\n",
    "#     - select the nearest k \n",
    "#     - don't forget to exlude distances between piunts \n",
    "\n",
    "# 3 A starts as a matrix of all zeros - it needs ones for the neighbours of i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_knn(X, k):\n",
    "    \"\"\" Finds k-nearest neighbours in X \"\"\"\n",
    "    N, D = X.shape\n",
    "    A = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        # 2.1 estimate the squared distances between point i and all neighbours\n",
    "        i_sq_distances = None\n",
    "        # 2.2 find the nearest points\n",
    "        nearest_points = None\n",
    "        # [1:k+1] is because the nearest point to i is i itself - but we don't want it\n",
    "        k_nearest      = None\n",
    "        for j in k_nearest:\n",
    "            A[i,j] = 1\n",
    "    return A\n",
    "\n",
    "A = my_knn(X, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of your k-nearest-neighbours function should be an NxN matrix, which has a 1 for nearest-neighbours and a 0 otherwise.\n",
    "\n",
    "Check that the function is working properly by running the below cell. This will plot a point (red) with its neighbours coloured in dark blue. \n",
    "\n",
    "**Are all these values clustered similarly to our results (demoed below)**\n",
    "\n",
    "\n",
    "<img src=\"imgs/neighbours.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "knn_i = A[i].nonzero()\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])\n",
    "_ = ax.scatter(X[:,0], X[:,1], X[:,2], c=X_m[:,1], marker='o')\n",
    "_ = ax.scatter(X[i,0], X[i,1], X[i,2], c='r', marker='o', s=300) # plot point i\n",
    "_ = ax.scatter(X[knn_i,0], X[knn_i,1], X[knn_i,2], c='b', marker='o', s=100) # plot i's nearest neighbours\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.scatter(X_m[:,0], X_m[:,1], c=X_m[:,1], marker='o')\n",
    "_ = ax.scatter(X_m[knn_i,0], X_m[knn_i,1], c='b', marker='o', s=100) # plot i's nearest neighbours\n",
    "_ = ax.scatter(X_m[i,0], X_m[i,1], c='r', marker='o', s=300) # plot point i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use in the Laplacian Eigenmaps method, this matrix needs to be symmetric.\n",
    "\n",
    "Does your knn algorithm always return a symmetric matrix? If not, we can symmetrise it. \n",
    "\n",
    "**To do** Run the cell below to return a symmetric your k-nearest neighbour _adjacency_ matrix $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetrise(X):\n",
    "    \"\"\" Symmetrises the matrix X.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    np.maximum returns the element-wise maximum of its arguments.\"\"\"\n",
    "    return np.maximum(X, X.T)\n",
    "A = symmetrise(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of the Laplacian Eigenmaps solution \n",
    "\n",
    "So, our aim is to find low-dimensional coordinates which maintain the near-neighbour relationships from the original data.\n",
    "\n",
    "We will say that we are trying to minimise the following cost function:\n",
    "\n",
    "$C = \\sum_{i,j} (y_i - y_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "Where $\\mathbf{A}_{ij}$ is our nearest-neighbour matrix which tells us whether two points are close together in the original dataset, and the $y_i$ and $y_j$ represent the low-dimensional coordinates we're trying to find.\n",
    "\n",
    "$\\mathbf{A}$ is fixed, and determined by our input data.\n",
    "$\\mathbf{y}$ is the solution we are trying to find.\n",
    "\n",
    "You can hopefully see that this <span style=\"color:red\"> </span> cost function is minimised if we place points which are close together in the original data (ie. $\\mathbf{A}_{ij}$ is high), close together in the new low-dimensional representation (so that $(x_i - x_j)^2$ is low). However, there are some trivial solutions we want to avoid. **Can you tell what these are?**\n",
    "\n",
    "**Answer** We seek a solution to $C = \\sum_{i,j} (y_i - y_j)^2 \\mathbf{A}_{ij}$, which avoids the trivial solution where all points get mapped onto a single location (i.e. either all $y_i=0$ or $y_i=y_j$ for all connected points).\n",
    "\n",
    "We can find a solution by multiplying out our cost function and substituting for $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$.\n",
    "\n",
    "$C = \\sum_{i,j} (y_i - y_j)^2 \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\sum_{i,j} (y_i^2 + y_j^2 - 2 y_i y_j) \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\sum_i y_i^2 \\mathbf{D_{ii}} +  \\sum_j y_j^2 \\mathbf{D_{jj}} - 2 \\sum_{i,j} y_i y_j \\mathbf{A}_{ij}$\n",
    "\n",
    "$C = \\mathbf{y^TDy} - \\mathbf{y^TAy}$\n",
    "\n",
    "$C = \\mathbf{y^TLy}$\n",
    "\n",
    "where $\\mathbf{L=D-W}$ is the graph laplacian. Thus we want to find the $\\mathbf{y}$ for which $\\mathbf{y^TLy}$ are minimised, subject to the contraints that $y_i\\neq0$ or $y_i \\neq y_j$. This is given by the eigenvectors of the graph Laplacian.\n",
    "\n",
    "#### Optional explaination\n",
    "\n",
    "For the first constraint $y_i\\neq0$, this can be solved by writing the constrained minmisation as the unconstrained problem:\n",
    "\n",
    "$$\\min_{ùëì\\neq 0 \\in \\mathbb{R}^n} \\frac{\\mathbf{y^TLy}}{\\mathbf{y^Ty}}$$\n",
    "\n",
    "Which is known as a Rayleigh quotient for which the solution is the eigenvector corresponding to the smallest eigenvalue of $L$. However, the smallest eigenvalue of the graph laplacian is zero and it corresponds to the constant eigenvector $\\mathbb{1}$. This also is undesirable as it corresponds to the solution $y_i=y_j$; \n",
    "\n",
    "## _Essential_ knowledge\n",
    "\n",
    "Under the following assumptions:\n",
    "1. you have a fully connected graph (and thus only one zero eigenvalue) \n",
    "2. you want a 2D embedding\n",
    "\n",
    "**The solution to Laplacian Eigenmaps is given by the eigenvectors corresponding to the second and third smallest eigenvalues of the graph Laplacian** \n",
    "\n",
    "**In the generic case**, for an N-D embedding, **you must take N eigenvector corresponding to the $N$ eigenvalues after the last 0 eigenvalue**\n",
    "\n",
    "Importantly, note **unlike** PCA there is no need to project the data onto the eigenvectors - **the eigenvectors ARE the embeddding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - implement the Laplacian Eigenmaps embedding of the swiss roll data set\n",
    "\n",
    "Write a function that takes in an adjacency matrix and implements Laplacian eigenmap method described above.\n",
    "\n",
    "**To do** \n",
    "1. Get $\\mathbf{A}$ using functions ```my_knn``` and ```symmetrise```\n",
    "2. Get $\\mathbf{D}$ from $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$ \n",
    "  \n",
    "3. Get $\\mathbf{L}$ from $\\mathbf{D}$ and $\\mathbf{A}$\n",
    "4. complete the function below to extract the correct eigenvectors from the Laplacian and plot the embedding. \n",
    "  - see hints above - we only want d eigenvectors corresponding to the d smallest eigenvalues (above 0)\n",
    "  \n",
    "*Hints**\n",
    "- $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$  requires ```np.sum``` and ```np.diag```\n",
    "- scipy has a function called [scipy.linalg.eigh](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html) which can help you fund the eigenvectors of a _symmetric_ matrix efficiently (it returns them sorted in ascending order)\n",
    "- you can supply argument ```'subset_by_index'```  to scipy.linalg.eigh to control the range of eigenvalue/eigenvector pairs returned *** which eigenvalue eigenvectors do you need if we are looking for the d  eigenvectors corresponding to the s smallest eigenvalues (excluding zero eigenvalues**) s\n",
    "\n",
    "    \n",
    "**Then consider**\n",
    "- Can your method recover the original manifold structure? \n",
    "- Try changing the the number of nearest neighbours $k$. \n",
    "- What happens when $k$ is set very high, or very low?\n",
    "\n",
    "**Hidden cell below has brief additional nodes on scipy.linalg.eigh if you are still stuck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_laplacian_eigenmap(X, k=20, d=2):\n",
    "    # 3.1 a use function my_knn to return A\n",
    "    A = None\n",
    "    # 3.1 b use function symmetrise to make A symmetric\n",
    "    A = None\n",
    "    # 3.2 create diagonal matrix D from A\n",
    "    D = None\n",
    "    # 3.3 create L\n",
    "    L = None\n",
    "    # 3.4 return eigenvectors of L using np.eigh\n",
    "    v, X = None\n",
    "    return X\n",
    "\n",
    "Z = my_laplacian_eigenmap(X)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.scatter(Z[:,0], Z[:,1], c=X_m[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) The normalised graph Laplacian\n",
    "\n",
    "Strictly, Laplacian Eigenmaps do not use $\\mathbf{L=D-W}$, but instead seek eigenvectors of the normalised graph Laplacian - the solutions to the generalised eigenvector problem $\\mathbf{Ly}=\\lambda\\mathbf{Dy}$\n",
    "\n",
    "If the graph is very regular and most vertices have approximately the same degree, then the normalised Laplacian and unnormalised Laplacian will generate very similar results. However, if the degrees in the graph are very broadly distributed it is more stable to use a normalised Laplacian $\\mathbf{L=D^{-1/2}(D-A)D^{-1/2}}$. By default this is the approach taking by Scikit Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Implementing Laplacian Eigenmaps with Scikit Learn\n",
    "\n",
    "Another name for Laplacian Eigenmaps is [spectral embedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding) . This is because the Graph Laplacian can be seen as a discrete version of the Laplace operator which describes diffusion on a surface. And, by looking at the eigenvectors (or spectra) of this matrix (equivalent to the eigenmodes of the Laplace operator) we can see the most significant modes of variation in the graph representing our original data.\n",
    "\n",
    "Laplacian Eigenmaps may therefore be implemented in scikit learn using the ```SpectralEmbedding``` function from the ```manifold``` module. \n",
    "\n",
    "**To do** Using what you know about the consistency of the scikit-learn API implement laplacian eigenmaps using sklearn. Replace ```None``` in cell below. You can use the link above for guidance\n",
    "\n",
    "**hint** you will need to set the ```n_components``` and ```n_neighbour``` arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "model=None\n",
    "\n",
    "Z_2 = None\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "_ = ax.scatter(Z_2[:,0], Z_2[:,1], c=X_m[:,1], marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
