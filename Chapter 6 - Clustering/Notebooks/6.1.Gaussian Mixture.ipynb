{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture\n",
    "\n",
    "In this notebook we will explore Gaussian Mixture Model. We will first apply it to high-dimensional Winsconsin breast cancer dataset and compare it to K-means clustering. Then we will also apply it to segmentation of brain MRI into White Matter (WM), Gray Matter (GM) and Cerebro-spinal Fluid (CSF), and explore its various components, such as image and class intensity distributions, also called **likelihoods**, and resulting probabilistic predictions, also called **posteriors**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Breast Cancer Dataset\n",
    "\n",
    "We will first load the dataset and create the feature matrix `X` and the label vector `y`. Note that the label vector will only be used for evaluation of the clustering result, because clustering is an unsupervised machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "print('\\n Features: \\n', bc.feature_names)\n",
    "print('\\n Labels: ', bc.target_names)\n",
    "\n",
    "# Feature matrix\n",
    "X = bc.data\n",
    "# Label vector\n",
    "y = bc.target\n",
    "print('\\n We have {} features.'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have seen in this module that we can use dimensionality reduction to visualise structure of the high-dimensional dataset. We will use `PCA` to visualise our 30-dimensional dataset.\n",
    "\n",
    "**Activity 1:** Complete the code below to explore structure of the Wisconsin Breast Cancer Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# apply standard scaler to the feature matrix\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# create PCA feature transformer with 2 components\n",
    "model = None\n",
    "\n",
    "# transform features using PCA\n",
    "X2 = None\n",
    "\n",
    "# create function for plotting 2D two-class dataset\n",
    "def PlotData(X,y):\n",
    "    # plot\n",
    "    plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.5, label = bc.target_names[0])\n",
    "    plt.plot(None,None,'r*',alpha=0.5, label = bc.target_names[1])\n",
    "    # annotate\n",
    "    plt.legend()\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.title('Wisconsin Breast Cancer Dataset')\n",
    "\n",
    "# Plot reduced dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "\n",
    "Let's cluster the dataset using K-means and see how it well it can distinguish natural clusters in the dataset and whether these clusters correpond to the heathy and cancerous cells.\n",
    "\n",
    "**Activity 2:** Complete the code below to perform k-means clustering with 2 clusters and evaluate the accuracy compared to the ground truth labels. Did the 30D model perform better than the 2D models in the lecture? \n",
    "\n",
    "*Note: We need to print out two different scores because the clusters will be assigned labels 0 or 1 randomly. The higher score will be the measure of the performance.*\n",
    "\n",
    "**Answer:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create k-means model with 2 clusters\n",
    "model = None\n",
    "\n",
    "# Fit the model and predict the labels\n",
    "y_pred = None\n",
    "\n",
    "# Print out accuracy score compared to ground truth labels\n",
    "print('Accuracy score: ', round(accuracy_score(y,y_pred),2))\n",
    "print('Accuracy score: ', round(accuracy_score(y,1-y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the clustering result. \n",
    "\n",
    "**Activity 3:** Call the function `PlotData` with the predicted labels. Compare to the ground truth labels plotted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clustering result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 \n",
    "### Clustering high-dimensional dataset using Gaussian Mixture\n",
    "\n",
    "Your task is now to fit Gaussian Mixture model to the 30D dataset and compare it performance to the K-means. Perform following tasks:\n",
    "* Create GMM model with 2 clusters\n",
    "* Fit the model and predict the labels\n",
    "* Print out accuracy score compared to ground truth labels\n",
    "* Plot the clustering result\n",
    "Which method performed better. Can you reason why?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Create GMM model with 2 clusters\n",
    "model = None\n",
    "\n",
    "# Fit the model and predict the labels\n",
    "y_pred = None\n",
    "\n",
    "# Print out accuracy score compared to ground truth labels\n",
    "print('Accuracy score: ', round(accuracy_score(y,y_pred),2))\n",
    "print('Accuracy score: ', round(accuracy_score(y,1-y_pred),2))\n",
    "\n",
    "# Plot the clustering result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load brain MRI\n",
    "Our 2D brain MRI image is saved in a pickle format as 'slice.p'. The non-brain tissue has been removed and image has been padded with zeros. When performing GMM clustering to segment the WM, GM and CSF, we will need to exclude the zero values for the algorithm to work well. \n",
    "\n",
    "Run the code bellow to load and plot the brain MRI slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "slice = pickle.load(open( \"datasets/slice.p\", \"rb\" ))\n",
    "print('Slice dimesions: ',slice.shape)\n",
    "\n",
    "plt.imshow(slice)\n",
    "plt.set_cmap('gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now first plot the histogram of this image. We see the large peak of zero values which is the padding of the background.\n",
    "\n",
    "*Note: we use* `_=plt.hist` *because we do not want the histogram values to be printed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display histogram\n",
    "_=plt.hist(slice.flatten(), bins = 100, density = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to exclude the padding to make the GMM segmentation work well. We therefore select only non-zero elements and plot histogram of these. This can be done using function `np.where` (otherwise remebering the logistic array `slice>0` is fine too). Note that `slice2`, which contains the selected pixels is now a 1D array. \n",
    "\n",
    "Can you recognise the peaks of WM, GM and CSF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indices of non-zero elements\n",
    "ind = np.where(slice>0)\n",
    "# select non-zero elemenst\n",
    "slice2 = slice[ind]\n",
    "# check the dimension\n",
    "print('Shape od selected data is ', slice2.shape)\n",
    "# plot histogram\n",
    "_=plt.hist(slice2, bins = 100, density = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### Segmentation of brain MRI using Gaussian mixture\n",
    "\n",
    "__Task 2.1:__ Now perform clustering of `slice2` using `GaussianMixture`. Set number of clusters to `3` and random state to `42` to get the same result every time you rerun it. Check help how exactly to do that. Perform following tasks:\n",
    "* Create the `GausianMixture` model\n",
    "* Create the feature matrix `X` by reshaping `slice2` into 2D array\n",
    "* Fit the model and predict the labels\n",
    "* Reshape the predicted labels to the original shape of `slice`\n",
    "* Display using `imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# select model\n",
    "model=None\n",
    "\n",
    "# create features - needs to be a 2D array\n",
    "X = None\n",
    "\n",
    "# fit the model and predict the cluster labels \n",
    "y_pred = None\n",
    "\n",
    "# Create array of 2D labels\n",
    "labels2D = np.zeros(slice.shape)\n",
    "\n",
    "# put the labels into fields with non-zero indices\n",
    "# You need to add one so that labels start from 1\n",
    "labels2D[None]=None+1\n",
    "\n",
    "# display the label image\n",
    "\n",
    "plt.set_cmap('plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2:** Predict the probabilistic segmentations for each tissue class.  Display the maps in a figure with three subplots. \n",
    "\n",
    "To do that perform the following steps:\n",
    "* predict the probabilistic segmentations using function `predict_proba`\n",
    "* check the size of the resulting predicted probability matrix\n",
    "* write a `for` loop over the tissue types\n",
    "* select the probability map for the current class from the predicted probability matrix\n",
    "* create an array of zeros the same shape as `slice`\n",
    "* insert the class-dependent probability into the right locations in this array\n",
    "* display the array using `subplot` and `imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilistic segmentations\n",
    "proba = None\n",
    "\n",
    "# check the dimensions\n",
    "print('Dimensions of proba ', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display\n",
    "plt.figure(figsize = [15,4])\n",
    "plt.set_cmap('gray')\n",
    "\n",
    "for i in range(3):\n",
    "    # take only posteriors for class i\n",
    "    post = None\n",
    "\n",
    "    # reshape to the 3D image\n",
    "    post2D = None\n",
    "    post2D[None]=None\n",
    "    \n",
    "    # display\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(post2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (optional)\n",
    "\n",
    "### Explore Gaussian Mixture model\n",
    "\n",
    "In this exercise we will explore the theoretical concepts of Gaussian mixture models, including **likelihoods** and **posteriors**. We will use the `GaussianMixture` model that we fitted to perform segmentation of brain MRI in Exercise 2.\n",
    "\n",
    "### Posterior probabilities\n",
    "\n",
    "Probabilitic segmentation $p_{ik}$ gives us probability that pixel $i$ to belong the class $k$. These are in fact **posterior probabilities** $$p(z_i=k|x_i, \\mu_k, \\sigma_k,c_k)$$ for the labels $z_i$ given the intensity value $x_i$ and parameters $\\mu_k, \\sigma_k,c_k$ of the Gaussian intensity distribution for class $k$.\n",
    "\n",
    "**Task 3.1:** Now let's plot how posterior probability for each class varies with pixel intensity value.  Fill in the missing code below to display the probability curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel intensity value range\n",
    "intensity_range = np.linspace(0, np.max(slice2),200)\n",
    "\n",
    "# predict posterior probabilities for the intensity range\n",
    "# do not forget to reshape the intensity range to 2D array for the prediction\n",
    "proba_curves = None\n",
    "\n",
    "# display\n",
    "plt.figure(figsize = [14,10])\n",
    "# plot normalised histogram \n",
    "# normalisation is achieved by parameter density\n",
    "plt.subplot(211)\n",
    "_=plt.hist(slice2, bins = 100, density = True)\n",
    "plt.title('Normalised Intensity Histogram')\n",
    "\n",
    "# plot posterior probabilities in a for loop\n",
    "plt.subplot(212)\n",
    "for i in range(0,3):\n",
    "    plt.plot(None,None, linewidth = 3, label = 'Class {}'.format(i))\n",
    "\n",
    "# annotate the subplot\n",
    "plt.title('Posterior probabilities')\n",
    "plt.xlabel('intensity')\n",
    "plt.ylabel('posterior probability')\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-dependent likelihood\n",
    "\n",
    "Class-dependent likelihoods are modelled by Gaussian distributions scaled by the mixing proportions\n",
    "$$p(x_i|z_i=k,\\mu_k,\\sigma_k, c_k)=G(x_i,\\mu_k,\\sigma_k)c_k$$ \n",
    "\n",
    "To display these distributions over the normalised histogram (use parameter `density=True`) we need to extract the `means_`, `covariances_` and `weights_` from the fitted `model`. Then we need to calculate the Gaussian distributions for these parameters. To do that we use function `norm.pdf` from `scipy.stats` module. Finally these distributions need to be multiplied by the weights and plotted.\n",
    "\n",
    "**Task 3.2:** Plot the Gaussian intensity distributions for each class $k$ over the normalised image histogrom. To do that, fill in the missing code below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate gaussian distribution\n",
    "from scipy.stats import norm\n",
    "\n",
    "# get parameters of GMM\n",
    "# use flatten to make 1D arrays\n",
    "\n",
    "# means\n",
    "m = None\n",
    "\n",
    "# standard deviation (you need to take sqrt of covariances)\n",
    "s = None\n",
    "\n",
    "# mixing proportions\n",
    "w = None\n",
    "\n",
    "# display\n",
    "plt.figure(figsize = [14,5])\n",
    "\n",
    "# histogram\n",
    "_=plt.hist(None, bins = 100, None)\n",
    "\n",
    "# class-dependent likelihoods - Gaussian PDFs\n",
    "for i in range(0,3):\n",
    "    likelihood = None\n",
    "    plt.plot(intensity_range,likelihood, linewidth = 3, label = 'Class {}'.format(i))\n",
    "plt.legend()\n",
    "plt.title('Class specific likelihood functions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "The likelihood for each pixel intensity $x_i$ given the Gaussian Mixture Model parameters $\\phi = (\\mu_k,\\sigma_k,c_k),k=1,...,K$ can be evaluated as\n",
    "$$p(x_i|\\phi)=\\sum_{k=1}^KG(x_i,\\mu_k,\\sigma_k)c_k $$\n",
    "\n",
    "We can calculate this function by adding the class-dependent likelihoods together. Alternative is to use the function `score_samples` provided by `GaussianMixture` model  that returns **log-likelihood**. \n",
    "\n",
    "**Task 3.3:** Plot the likelihood function for the whole intensity range over the normalised image histogram. To do that, first evaluate log-likelihood over the intensity range using fuction `score_samples`, then calculate the exponential using `np.exp` and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare histogram with fitted Gaussian mixture likelihood function\n",
    "plt.figure(figsize = [14,5])\n",
    "# histogram\n",
    "_=plt.hist(None, None, None)\n",
    "# calculate likelihood\n",
    "log_likelihood = None\n",
    "likelihood = np.exp(None)\n",
    "# plot likelihood\n",
    "plt.plot(intensity_range, None, linewidth = 3, c='k')\n",
    "plt.title('Fitted likelihood function')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
