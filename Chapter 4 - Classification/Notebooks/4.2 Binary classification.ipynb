{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "\n",
    "In this notebook we will practise binary classification in 2D using the example of Logistic regression. We will use the example of prediction of heart failure that you are already familiar with. We will also compare binary classifiers with different pairs of features to see which markers are best predictive of the disease.\n",
    "\n",
    "First we need to import the libraries. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "Now we will load the dataset. You can see that we have the diagnosis in column **HF**, with labels `0`, `1` and `2`, and three features, **EF**, **GLS** and **QRS**. Run the cells below to load and explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read fine into a dataframe object\n",
    "df = pd.read_csv('datasets/heart_failure_data_complete.csv')\n",
    "# print the first few lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print labels\n",
    "print('Labels: ', pd.unique(df['HF']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit a binary classifier we will only consider 2 classes:\n",
    "* Healthy with label `0`\n",
    "* HF with label either `1` or `2`\n",
    "\n",
    "In the first part of this notebook we will select features **EF** and **GLS**. \n",
    "\n",
    "Run the code below to create the feature matrix `X` and label vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to numpy array\n",
    "data = df.to_numpy()\n",
    "\n",
    "# Create feature matrix with EF and GLS\n",
    "X = data[:,[1,2]]\n",
    "\n",
    "# scale the feature matrix\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create label vector with classes 0 and 1\n",
    "y = data[:,0]\n",
    "y[y==2]=1\n",
    "\n",
    "# print properties\n",
    "print('Feature matrix X dimensions: ', X.shape)\n",
    "print('Target vector y dimensions: ', y.shape)\n",
    "print('Labels: ', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "**Activity 1:** Complete the function `PlotData` and call it to plot the features `X` with different markers based on labels `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotData(X,y):\n",
    "    # plot class 0\n",
    "    plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.75,markeredgecolor='k',label = 'Healthy')\n",
    "    # plot class 1\n",
    "    plt.plot(X[None,None,'rd',alpha=0.75,markeredgecolor='k',label = 'HF')\n",
    "    \n",
    "    # annotate the plot\n",
    "    plt.title('Diagnosis of Heart Failure')\n",
    "    plt.xlabel('EF')\n",
    "    plt.ylabel('GLS')\n",
    "    plt.legend()\n",
    "\n",
    "# call the function to plot the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and plot the logistic regression model\n",
    "\n",
    "In the cell below we fit the logistic regression model. Run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Plot the predicted model in 2D. Complete the function `PlotClassification` and run the code below to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotClassification(model,X,y):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "\n",
    "    # Predict labels for the whole feature space    \n",
    "    y_pred = None\n",
    "    # Resahpe to 2D\n",
    "    y_pred = y_pred.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, y_pred, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData(X,y)\n",
    "\n",
    "# Plot classification\n",
    "PlotClassification(model,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 3:** Complete function `PlotProbabilities` to plot the probability for the class 1. Change label form `1` to `0` to see the probability for class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotProbabilities(model,X,y,label=1):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "\n",
    "    # Predict labels for the whole feature space    \n",
    "    proba = None\n",
    "    # Select the class\n",
    "    p = proba[:,label]\n",
    "    # Resahpe to 2D\n",
    "    p = p.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, p, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData(X,y)\n",
    "\n",
    "# Plot probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 4:** Logistic regression model in scikit-learn includes a Ridge penalty, that is controlled by hyperparameter `C`. In the cell bellow, try different values of alpha to see the effect of Ridge penalty on the predicted probabilities of class 1. You can try for example `1000`, `1`, `0.001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha\n",
    "model = LogisticRegression(C = 1000)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X,y)\n",
    "\n",
    "# Plot probabilities\n",
    "PlotProbabilities(model,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of the classifier\n",
    "\n",
    "The simplest way to evaluate performance of the classifier is using **accuracy score**. This score is a good evaluation measure for **balanced** datasets, where the number of samples of each class is similar. \n",
    "\n",
    "*Note: We only calculate cross-validated performance meansures in the examples below. This is fine, as long as no parameters are tuned. If we also tune parameters, the performance of the classifier need to be evaluated on test set.*\n",
    "\n",
    "**Actvity 5:** Check whether our dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples of class 0: ', y[y==0].shape[0])\n",
    "print('Number of samples of class 1: ', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 6:** Calculate the cross-validated accuracy for the default model. To do that complete the the function `Accuracy_CV`. \n",
    "\n",
    "*Hint:* Use function `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross-validated accuracy\n",
    "def Accuracy_CV(model,X,y):\n",
    "    scores = None\n",
    "    print('Accuracy CV: ',round(scores.mean(),2))\n",
    "\n",
    "# select default Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X,y)\n",
    "\n",
    "# Evaluate accuracy\n",
    "Accuracy_CV(model,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the balanced datasets, the accuracy is a good measure. For balanced binary dataset, accuracy higher than 0.5 means that the classifier managed to learn some information about the data (0.5 corresponds to a random assignment of the labels). We can check this for our dataset by calculating also **sensitivity** and **specificity**, and check that they are both high.\n",
    "\n",
    "**Activity 7:** Calculate sensitivity and specificity for the fitted model. \n",
    "\n",
    "*Hint:* Use function `recall_score`. Remember from the lecture, that recall for positive label set to 1 is sensitivity, while recall for positive label set to 0 is specificity. Positive label is set using argument `pos_label`. Average recall can be calculated by setting `average='macro'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Predict labels using cross-validation\n",
    "y_pred = cross_val_predict(model,X,y)\n",
    "\n",
    "# Sensitivity\n",
    "sensitivity = recall_score(y,y_pred,pos_label = 1)\n",
    "print('Sensitivity: ',round(sensitivity,2))\n",
    "\n",
    "# Specificity\n",
    "specificity = None\n",
    "print('Specificity: ',round(specificity,2))\n",
    "\n",
    "# Average recall\n",
    "mean_recall = recall_score(y,y_pred,average=None)\n",
    "print('Mean Recall: ',round(mean_recall,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/QRS.png\" width = \"200\" style=\"float: right;\"> \n",
    "## Exercise 2: Compare classifiers\n",
    "\n",
    "Global longitudinal strain is difficult to estimate correctly due to manual measurements that need to be performed, resulting in poor reproducibility. Researchers believe, that GLS could be replaces by duration of QRS interval extracted from ECG, that is much easier to measure. \n",
    "\n",
    "Your task is to compare the classifier that predicts heart failure from EF and GLS with the classifier that predict HF from EF and QRS.\n",
    "\n",
    "### Create dataset with EF and QRS\n",
    "Run the code bellow to create a feature matrix `X2` that contains EF and QRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature matrix with EF and QRS\n",
    "X2 = data[:,[1,3]]\n",
    "X2 = scaler.fit_transform(X2)\n",
    "\n",
    "# Plot the new dataset\n",
    "PlotData(X2,y)\n",
    "plt.ylabel('QRS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Fit and plot the model\n",
    "\n",
    "Fit the default Logistic regression model to the EF QRS dataset and plot the classification results using function `PlotClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the model\n",
    "model2 = None\n",
    "\n",
    "# fit the model\n",
    "\n",
    "\n",
    "# plot classification results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Performance\n",
    "\n",
    "Complete the function `EvaluatePerformance` to calculate **cross-validated**\n",
    "* accuracy\n",
    "* sensitivity\n",
    "* specificity\n",
    "* average recall\n",
    "\n",
    "Run the function to print out the performance measures and compare to the EF GLS dataset. Which of the dataset is better for prediction of heart failure?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "*Note: We are not tuning any parameters here, so CV score is suitable for final evaluation of the performance. However, as soon as we tune the parameters, final performance has to be evaluated on the test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluatePerformance(model,X,y):\n",
    "    \n",
    "    # accuracy\n",
    "    scores = None\n",
    "    print('Accuracy: ', round(scores.mean(),2))\n",
    "    \n",
    "    # Predict labels using cross-validation\n",
    "    y_pred = None\n",
    "\n",
    "    # Sensitivity\n",
    "    sensitivity = None\n",
    "    print('Sensitivity: ',round(sensitivity,2))\n",
    "\n",
    "    # Specificity\n",
    "    specificity = None\n",
    "    print('Specificity: ',round(specificity,2))\n",
    "\n",
    "    # Average recall\n",
    "    mean_recall = None\n",
    "    print('Mean Recall: ',round(mean_recall,2))\n",
    "\n",
    "          \n",
    "# Calculate performance for EF QRS dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Tuning the Ridge penalty (optional)\n",
    "\n",
    "Find out whether the performance of the EF QRS model can be further improved by tuning the hyperparameter `C` that controls the strength of Ridge regularisation. \n",
    "* Use function `GridSearchCV` to tune `C`\n",
    "* Print out the best parameter `C` and best score\n",
    "* Plot the classification boundary and the probabilites for class 1\n",
    "* Calculate performance all measures for the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# parameter grid\n",
    "param = {'C':np.logspace(-3,3,7)}\n",
    "\n",
    "# grid search \n",
    "g = None\n",
    "g.fit(None,None)\n",
    "\n",
    "# best model\n",
    "best_model = None\n",
    "\n",
    "# print best accuracy score\n",
    "print('Accuracy: ', round(g.best_score_,2))\n",
    "\n",
    "# print best C\n",
    "print('C: ', best_model.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability for class 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate performance measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the performance of the tuned Logistic Regression better than default?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "*Note: Because we are tuning parameters here, CV score is **not** suitable for final evaluation of the performanceand test set is needed for that.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
