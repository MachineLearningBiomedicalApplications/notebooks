{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "In this notebook we will consider three classes of patients:\n",
    "* Healthy: Label 0\n",
    "* Mild Heart Failure: Label 1\n",
    "* Severe Heart Failure: Label 2\n",
    "\n",
    "We will consider two features - **EF** and **GLS**.\n",
    "\n",
    "### Libraries and dataset\n",
    "Run the code below to load the libraries, dataset, create the feature matrix `X` and label vector `y` and finally plot the dataset. Note that we updated the function `PlotData` to deal with 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# read fine into a dataframe object\n",
    "df = pd.read_csv('datasets/heart_failure_data_complete.csv')\n",
    "# convert dataframe to numpy array\n",
    "data = df.to_numpy()\n",
    "# Create feature matrix with EF and GLS\n",
    "X = data[:,[1,2]]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# Create label vector \n",
    "y = data[:,0]\n",
    "# print properties\n",
    "print('Feature matrix X dimensions: ', X.shape)\n",
    "print('Target vector y dimensions: ', y.shape)\n",
    "print('Labels: ', np.unique(y))\n",
    "\n",
    "def PlotData(X,y):\n",
    "    # plot class 0\n",
    "    plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.75,markeredgecolor='k',label = 'Healthy')\n",
    "    # plot class 1\n",
    "    plt.plot(X[y==1,0],X[y==1,1],'rd',alpha=0.75,markeredgecolor='k',label = 'Mild HF')\n",
    "    # plot class 2\n",
    "    plt.plot(X[y==2,0],X[y==2,1],'g^',alpha=0.75,markeredgecolor='k',label = 'Severe HF')\n",
    "    \n",
    "    # annotate the plot\n",
    "    plt.title('Diagnosis of Heart Failure')\n",
    "    plt.xlabel('EF')\n",
    "    plt.ylabel('GLS')\n",
    "    plt.legend()\n",
    "\n",
    "# call the function to plot the dataset\n",
    "PlotData(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "The cell below contains some of the functions that we have created in the previous notebook and which can be reused for multilabel classification. These functions are\n",
    "* `PlotClassification`\n",
    "* `PlotProbabilities`\n",
    "* `Accuracy_CV`\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotClassification(model,X,y):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "\n",
    "    # Predict labels for the whole feature space    \n",
    "    y_pred = model.predict(Feature_space)\n",
    "    # Resahpe to 2D\n",
    "    y_pred = y_pred.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, y_pred, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData(X,y)\n",
    "\n",
    "def PlotProbabilities(model,X,y,label=1):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "\n",
    "    # Predict labels for the whole feature space    \n",
    "    proba = model.predict_proba(Feature_space)\n",
    "    # Select the class\n",
    "    p = proba[:,label]\n",
    "    # Resahpe to 2D\n",
    "    p = p.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, p, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData(X,y)\n",
    "    \n",
    "def Accuracy_CV(model,X,y):\n",
    "    scores = cross_val_score(model,X,y)\n",
    "    print('Accuracy CV: ',round(scores.mean(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and plot the model\n",
    "We will first fit the logistic regression model to the data. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 1:** Plot the classification labels using `PlotClassification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Plot probabilities for all three classes in the same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,3.5))\n",
    "classes = ['Healthy','Mild Heart Failure', 'Severe Heart Failure']\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    # plot probabilities\n",
    "    \n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "**Activity 3:** Evaluate cross-validated accuracy of the model using `Accuracy_CV`. Is the performance good? To see whether accuracy is reliable measure of performance, calculate the number of samples in each class. Is the dataset balanced?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "\n",
    "\n",
    "# check for class imbalance\n",
    "print('Number of samples of class 0: ', y[y==0].shape[0])\n",
    "print('Number of samples of class 1: ', None)\n",
    "print('Number of samples of class 2: ', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "The function `Recall_CV` calculates different type of recalls that help us analyse the performance of the multilabel classifier. These can be set by parameter `average`:\n",
    "\n",
    "* `average=None` will return recall for each class. We can find out how well the classifier indentifies each class.\n",
    "* `average=macro` will return recall average over the classes. This measure is sensitive to the performance of individual classes irrespective of their sizes.\n",
    "* `average=micro` will return recall averaged over the samples, e.g. the recall is calculated globally by counting the total true positives, false negatives and false positives. This measure is not very sensitive to the classes of small sizes.\n",
    "\n",
    "**Activity 4**: Calculate different types of recalls by calling the function `Recall_CV`. What can you tell about the performance of the classifier for different classes? How do the different average recalls compare to the accuracy score?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recall_CV(model,X,y):\n",
    "    # predict labels using cross-validation\n",
    "    y_pred = cross_val_predict(model,X,y)\n",
    "    \n",
    "    # calculate recalls for all classes\n",
    "    recalls = recall_score(y,y_pred,average=None)\n",
    "    print('Recalls for all classes: ', np.around(recalls,2))\n",
    "    \n",
    "    # calculate recall averaged over classes\n",
    "    mean_recall_macro = recall_score(y,y_pred,average='macro')\n",
    "    print('Average Recall macro: ', np.around(mean_recall_macro,2))\n",
    "    \n",
    "    # calculate recall averaged over samples\n",
    "    mean_recall_micro = recall_score(y,y_pred,average='micro')\n",
    "    print('Average Recall micro: ', np.around(mean_recall_micro,2))\n",
    "\n",
    "# Call function to calculate recalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "The code below calculate and plots the confusion matrix. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels using cross-validation\n",
    "y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "# calculate confusion matrix\n",
    "cm = confusion_matrix(y, y_pred);\n",
    "print('Confusion matrix: ')\n",
    "print(cm)\n",
    "\n",
    "# plot confusion matrix\n",
    "plt.matshow(cm, cmap='gray')\n",
    "plt.xlabel('Predicted labels', fontsize = 16)\n",
    "plt.ylabel('True labels', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 5:** Set the diagonal to zero and describe the main sources of misclassification.\n",
    "\n",
    "**Answer:** The main sources of misclassification are\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set diagonal to zero\n",
    "np.fill_diagonal(cm, 0)\n",
    "\n",
    "# Plot matrix\n",
    "plt.matshow(cm, cmap='gray')\n",
    "plt.xlabel('Predicted labels', fontsize = 16)\n",
    "plt.ylabel('True labels', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Balanced multi-label classification \n",
    "\n",
    "One way to deal with class imbalance is to increase the weight of the samples of smaller classes in the classification loss. For `LogisticRegression` this can be done by setting `class_weight='balanced'`, which will weight the sample inversly to the class size, this giving equal importance to each class irrespective of its size. \n",
    "\n",
    "In this exercise you will fit the logistic regression with class weighting and analyse the performance of the balanced model.\n",
    "\n",
    "### Task 1: Fit and plot the balanced model\n",
    "\n",
    "Fit the logistic regression with `class_weight='balanced'` and plot the classification boundaries/labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the balanced logistic regression model\n",
    "model2 = None\n",
    "\n",
    "# fit the model\n",
    "\n",
    "\n",
    "# plot classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Accuracy\n",
    "Calculate the cross-validated accuracy of the balanced model. Was it affected by the change in weighting of the samples?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CV accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Recall\n",
    "\n",
    "Calculate different types of cross-validated recalls. What has change in the performance of the classifier compared to the unballanced version?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Recalls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Confusion matrix\n",
    "Calculate and plot the confusion matrix to identify main types of misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict labels using cross-validation\n",
    "y_pred = None\n",
    "\n",
    "# calculate confusion matrix\n",
    "cm = None\n",
    "print('Confusion matrix: ')\n",
    "print(cm)\n",
    "\n",
    "# plot confusion matrix\n",
    "plt.matshow(cm, cmap='gray')\n",
    "plt.xlabel('Predicted labels', fontsize = 16)\n",
    "plt.ylabel('True labels', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set diagonal to zero\n",
    "np.fill_diagonal(cm, 0)\n",
    "\n",
    "# Plot matrix\n",
    "plt.matshow(cm, cmap='gray')\n",
    "plt.xlabel('Predicted labels', fontsize = 16)\n",
    "plt.ylabel('True labels', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The main types of misclassifications are:\n",
    "* \n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
