{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector classification\n",
    "\n",
    "In this notebook we will explore Support Vector Classifier (SVC). Linear binary SVC is very similar to the perceptron and logistic regression in a sense that it finds the optimal hyperplane to separate two classes. These methods, however, have different objectives through which they decide what is the optimal decision boundary.\n",
    "\n",
    "There are three different SVC classifiers in `sklearn` library:\n",
    "1. `LinearSVC` implements linear classifier optimised for performance but does not support the kernel trick\n",
    "2. `SVC` implements SVC with kernel trick. Setting `kernel='linear'` produces the same result as `LinearSVC` but is less efficient in terms of computational time. Setting `kernel='rbf'` produces non-linear classifier with Gaussian kernel.\n",
    "3. `SGDclassifier` implements various classifiers that are optimised using stochastic gradient descent. Its default setting for loss function is `loss='hinge'` which is another implementation of a linear SVC.\n",
    "\n",
    "SVC result also depends on hyperparameter `C` which controls the width of the margin and regularises the decision function. Larger `C` means smaller margin, less regularisation, and closer approximation of hard margin objective. Smaller `C` means larger margin, and smoother boundary for non-linear SVC. Note, that `C` has an opposite role to the parameter `alpha` that we used for penalised regression (e.g. `Ridge`). This is because it multiplies the data term rather than the penalty term.\n",
    "\n",
    "### Libraries \n",
    "\n",
    "Run the code below to import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will initially consider a  binary dataset with **Healthy** and **Heart Failure** patients, and with features **EF** and **QRS**. The code contains function `PlotData` that is suitable for both 2 and 3 label datasets.\n",
    "\n",
    "Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fine into a dataframe object\n",
    "df = pd.read_csv('datasets/heart_failure_data_complete.csv')\n",
    "# convert dataframe to numpy array\n",
    "data = df.to_numpy()\n",
    "# Create feature matrix with EF and QRS\n",
    "X = data[:,[1,3]]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# Create label vector \n",
    "y = data[:,0]\n",
    "y[y==2]=1\n",
    "# print properties\n",
    "print('Feature matrix X dimensions: ', X.shape)\n",
    "print('Target vector y dimensions: ', y.shape)\n",
    "print('Labels: ', np.unique(y))\n",
    "\n",
    "# Generalised for 2 or 3 of classes\n",
    "def PlotData(X,y):\n",
    "    # number of labels\n",
    "    c = (y.max()+1).astype(int)\n",
    "    # marker styles\n",
    "    m = ['bo','rd','g^']\n",
    "    # plot all classes\n",
    "    for k in range(0,c):\n",
    "        plt.plot(X[y==k,0],X[y==k,1],m[k],alpha=0.75,markeredgecolor='k')\n",
    "        \n",
    "    plt.title('Diagnosis of Heart Failure')\n",
    "    plt.xlabel('EF')\n",
    "    plt.ylabel('QRS')\n",
    "    \n",
    "    if c == 2:\n",
    "        plt.legend(['Healthy','Heart Failure'])\n",
    "    if c == 3:\n",
    "        plt.legend(['Healthy','Mild HF','Severe HF'])\n",
    "        \n",
    "# call the function to plot the dataset\n",
    "PlotData(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "The code below contains functions for plotting the classification results and evaluation of performance. The are:\n",
    "* `PlotClassification` plots the predicted labels for the feature space\n",
    "* `PlotDecisionFunction` plots the decision boundary and margins. If `plotSV=True` it also plots the support vectors. If `plotDF=True` it also plots the decision function.\n",
    "* `EvaluatePerformance` calculates and prints accuracy, class-dependent recalls and class-averaged recalls using cross-validation.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotClassification(model,X,y):\n",
    "\n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "\n",
    "    # Predict labels for the whole feature space    \n",
    "    y_pred = model.predict(Feature_space)\n",
    "    # Resahpe to 2D\n",
    "    y_pred = y_pred.reshape(x1.shape)\n",
    "    # Plot using contourf\n",
    "    plt.contourf(x1, x2, y_pred, cmap = 'summer')\n",
    "    \n",
    "    # Plot data\n",
    "    PlotData(X,y)\n",
    "\n",
    "def PlotDecisionBoundary(model,X,y, plotSV = False, plotDF = False):\n",
    "        \n",
    "    # Create an 1D array of samples for each feature\n",
    "    x1 = np.linspace(-2.5, 2, 1000) \n",
    "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
    "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
    "    x1, x2 = np.meshgrid(x1, x2) \n",
    "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
    "    Feature_space = np.c_[x1.ravel(), x2.ravel()] \n",
    "\n",
    "    # Predict decision function\n",
    "    df = model.decision_function(Feature_space)\n",
    "    # Resahpe to 2D\n",
    "    df = df.reshape(x1.shape)\n",
    "    # Zero countour is decision boundary, isolines +-1 are the margins\n",
    "    contour = plt.contour(x1, x2, df,[-1,0,1],colors='k',linestyles=('dashed', 'solid', 'dashed'))\n",
    "    plt.clabel(contour, inline=1, fontsize=14)\n",
    "    # Plot decision function\n",
    "    if plotDF:\n",
    "        plt.contourf(x1, x2, df, cmap = 'summer')\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plotSV:\n",
    "        svs = model.support_vectors_\n",
    "        plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA', label = 'Support vectors',edgecolor='k')\n",
    "    \n",
    "    # plot data\n",
    "    PlotData(X,y)\n",
    "    \n",
    "def EvaluatePerformance(model,X,y):\n",
    "    \n",
    "    # accuracy\n",
    "    scores = cross_val_score(model,X,y)\n",
    "    print('Accuracy: ', round(scores.mean(),2))\n",
    "    \n",
    "    # Predict labels using cross-validation\n",
    "    y_pred = cross_val_predict(model,X,y)\n",
    "\n",
    "    # calculate recalls for all classes\n",
    "    recalls = recall_score(y,y_pred,average=None)\n",
    "    print('Recalls for all classes: ', np.around(recalls,2))\n",
    "    \n",
    "    # calculate recall averaged over classes\n",
    "    mean_recall_macro = recall_score(y,y_pred,average='macro')\n",
    "    print('Average Recall macro: ', np.around(mean_recall_macro,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC\n",
    "\n",
    "We will first fit linear SVC using `LinearSVC` to our dataset.\n",
    "\n",
    "**Activity 1:** The code below fits the default linear SVC to the dataset. Perform following\n",
    "* Plot classification result using `PlotClassification`\n",
    "* Calculate performance metrics using `EvaluatePerformance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = LinearSVC()\n",
    "# fit the model\n",
    "model.fit(X,y)\n",
    "# Plot classification result\n",
    "\n",
    "# Evaluate performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Plot decision boundary and the margins using `PlotDecisionBoundary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision boundary and margins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vectors\n",
    "LinearSVC does not return the support vectors. We can instead use kernel `SVC` with `kernel='linear'`.\n",
    "\n",
    "**Activity 3:** Plot the decision boundary and support vectors by calling `PlotDecisionBoundary` with parameter `plotSV=True`.\n",
    "\n",
    "**Activity 4:** Set the parameter `C` to different values and plot the decision boundary and margin. Observe how margin increases with smaller `C`. You can try values `10`,`1`,`0.1` and `0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = SVC(kernel='linear')\n",
    "# fit the model\n",
    "model.fit(X,y)\n",
    "# plot decision boundary with support vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVC\n",
    "\n",
    "Now we will explore non-linear kernel SVC.\n",
    "\n",
    "**Activity 5:** Perform following tasks:\n",
    "* Fit the kernel `SVC` model as in Activity 3, but this time set `kernel='rbf'` which is the Gaussian kernel. \n",
    "* Plot the decision boundary and margins.\n",
    "* Plot also the decision function by setting `plotDF=True`\n",
    "* The kernel size is controlled by parameter `gamma`. Vary the parameter values to see the effect on a decision boundary. You can try values `10`,`1`,`0.1` and `0.01`.\n",
    "* Delete the parameter `gamma` to restore it to the default value. Now vary values of parameter `C` to see the effect on the non-linear decision boundary. You can try values `1000`, `10`, `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = SVC(kernel=None)\n",
    "# fit the model\n",
    "model.fit(X,y)\n",
    "# plot decision boundary and margins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Kernel SVC\n",
    "\n",
    "In this exercise we will tune Kernel SVC to diagnose mild and severe heart failure from **EF** and **QRS**. We will tune and compare `SVC` with linear and Gaussian kernel.\n",
    "\n",
    "### Task 1: Dataset\n",
    "First we will update the dataset. The feature matrix is unchanged, but we need to create the label vector `y2` with all three original labels. The updated labels vector is created for you. Plot the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label vector with 3 labels\n",
    "data2 = df.to_numpy()\n",
    "y2 = data2[:,0]\n",
    "\n",
    "# Plot the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Tune SVC with Linear Kernel\n",
    "We will first fit the linear `SVC` with optimal parameter `C` found by grid search. Perform the following:\n",
    "* Complete the code to tune linear `SVC` model. What is the optimal `C`?\n",
    "* Evaluate the performance of this model using cross-validation. Does it perform badly for one of the classes?\n",
    "* Plot classification result using `PlotClassification`\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = SVC(kernel=None)\n",
    "# parameter grid\n",
    "param = {'C':np.logspace(-3,3,13)}\n",
    "# create grid search\n",
    "g = GridSearchCV(model,param,cv=5)\n",
    "# run the grid search\n",
    "g.fit(X,y2)\n",
    "# save best model\n",
    "modelLin = g.best_estimator_\n",
    "# print best C\n",
    "print('Best C: ',round(None,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of the fitted model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot classification result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Tune SVC with Gaussian kernel\n",
    "Now we will explore whether the non-linear classifier can improve performance. We will set the kernel to `rbf` to produce Gaussian kernel. We need to tune parameters `C` and `gamma` this time. Parameter `gamma` is related to the width of the Gaussian kernel by $\\gamma=\\frac{1}{2\\sigma^2}$.\n",
    "\n",
    "Perform the following:\n",
    "* Complete the code to tune Gaussian kernel `SVC`. What is the best `C` and `gamma`?\n",
    "* Evaluate performance of the tuned non-linear classifier using cross-validation. How does is compare to the linear one in terms of overall performance and for the worst performing class in linear `SVC`?\n",
    "* Plot the classification result\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = None\n",
    "# parameter grid\n",
    "param = {'C':np.logspace(-3,3,13),\n",
    "        'gamma':np.logspace(-3,3,13)}\n",
    "# create grid search\n",
    "g = None\n",
    "# run the grid search\n",
    "\n",
    "# save best model\n",
    "modelGauss = None\n",
    "# print best C\n",
    "print('Best C: ',round(None,2))\n",
    "# print best gamma\n",
    "print('Best gamma: ',round(None,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Full training and evaluation algorithm\n",
    "We found out that kernel SVC was the best performing classifier for this task. However we have not yet evaluated how well it generalises to unseen data. For simplicity, we will only evaluate overall accuracy in this task. To do that\n",
    "* Perform stratified train test split\n",
    "* Tune Gaussian kernel `SVC` to the training set and print best CV accuracy\n",
    "* Evaluate performance of the tuned classifier on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performed stratified train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Gaussian kernel SVC to the training set and print best CV accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of the tuned classifier on test set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
