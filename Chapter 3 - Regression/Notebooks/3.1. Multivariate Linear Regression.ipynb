{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "\n",
    "In this notebook we will look at **Multivariate Linear Regression** which seeks to fit the model\n",
    "\n",
    "$$y=w_0+w_1x_1+...+w_Dx_D$$\n",
    "\n",
    "where $(x_1,...,x_D)^T$ is the *feature vector*, $\\hat{y}$ is the *target value* and $(w_0,...,w_D)^T$ is the *weight vector* formed of parameters of the regression model that we seek to find.\n",
    "\n",
    "We will first investigate how it is implemented in `sklearn` and implement it ourselves in `numpy`. Then we will look at measuring the performance of regression methods and investigate how dimension of the feature vector influences the performance of the linear regression.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "<img src=\"pictures/BrainVolumes.png\" width = \"500\" style=\"float: right;\"> \n",
    "The biomedical application that we will investigate in this notebook is prediction of gestational age (GA) at scan from volumes of brain structures. The brain volumes come from the **Developing Human Connectomme Project** (dHCP, www.developingconnectome.org). We will use three different feature vectors:\n",
    "* **Whole brain volume - a single feature** \n",
    "* **Main brain structures - six features:** cortical gray matter, cortical white matter, myelinated subcortical white matter, subcortical gray matter, cerebellum and brainstem. \n",
    "* **All dHCP brain structures - 86 features** \n",
    "\n",
    "The segmentations were performed using dHCP processing pipeline (www.doi.org/10.1016/j.neuroimage.2018.01.054). The software is available for download here: www.github.com/MIRTK/DrawEM. The list of all labels is available here:www.github.com/MIRTK/DrawEM/blob/master/label_names/all_labels.csv. Please note we have excluded label 84. \n",
    "\n",
    "This notebook accompanies the **Part I and II** of the Lecture 3 **Regression**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression in Scikit-learn\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "We will first explore the multivariate linear regression using the dataset with 6 brain volumes for prediction of GA at scan. \n",
    "\n",
    "The code bellow loads the dataset, extract the feature matrix and the target vector, and scales the features using `StandardScaler`. This is implemented in the function `CreateFeaturesTargets`, which accepts a 'csv' file as an input and returns extracted feature matrix `X` that contains the volumes and target vector `y`with GA. \n",
    "\n",
    "\n",
    "__Activity 1:__ Fill in the code to print out the numbers of sample and features and run the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def CreateFeaturesTargets(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename,header=None)\n",
    "    \n",
    "    # convert from 'DataFrame' to numpy array\n",
    "    data = df.values\n",
    "\n",
    "    # Features are in columns one to end\n",
    "    X = data[:,1:]\n",
    "    \n",
    "    # Scale features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Labels are in the column zero\n",
    "    y = data[:,0]\n",
    "\n",
    "    # return Features and Labels\n",
    "    return X, y\n",
    "\n",
    "X,y = CreateFeaturesTargets('datasets/GA-brain-volumes-6-features.csv')\n",
    "\n",
    "print('Number of samples is', None)\n",
    "print('Number of features is', None)\n",
    "print('Number of targets is', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the dataset\n",
    "Run the code bellow to visualise individual features of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.scatter(X[:,i], y)\n",
    "        plt.xlabel('Scaled volumes')\n",
    "        plt.ylabel('GA at scan')\n",
    "        plt.title('Structure '+ str(i))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model using normal equation\n",
    "\n",
    "The code bellow fits the `LinearRegression` model to the data using **normal equation**. The performance is evaluated using the **Root Mean Squared Error** (RMSE). This is done first on the whole dataset using function `mean_squared_error` and then using cross-validation implemented by a sklearn function `cross_val_score`. Run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Choose the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Calculate RMSE on the whole set\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print('RMSE: {} weeks'.format(round(rmse,2)))\n",
    "\n",
    "# Calculate RMSE using cross-validation\n",
    "scores = cross_val_score(model,X,y, scoring='neg_mean_squared_error',cv=5)\n",
    "rmse_cv = np.sqrt(-np.mean(scores))\n",
    "print('RMSE_CV: {} weeks'.format(round(rmse_cv,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Notice the RMSE. In which units is it? We calculate RMSE on the whole dataset and using cross-validation. Which one is larger? We will later compare these errors to other regression methods.\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model using SGD\n",
    "We will now fit the model multivariate linear regression model using **stochastic gradient descent** implemented in `SGDRegressor`.\n",
    "\n",
    "**Activity 3:** In the previous cell you have seen how to calculate RMSE. Complete the functions\n",
    "* `RMSE` to calculate RMSE on the whole dataset\n",
    "* `RMSE_CV` to calculate cross-validated RMSE \n",
    "\n",
    "Run the code and compare the error to the fit using normal equation, that we have perfored before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(model,X,y):\n",
    "    model.fit(X,y)\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = None\n",
    "    print('RMSE: {} weeks'.format(round(rmse,2)))\n",
    "    return rmse\n",
    "\n",
    "def RMSE_CV(model,X,y):\n",
    "    scores = None\n",
    "    rmse_cv = None\n",
    "    print('RMSE_CV: {} weeks'.format(round(rmse_cv,2)))\n",
    "    return rmse_cv\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = SGDRegressor(loss=\"squared_loss\", penalty=None)\n",
    "rmse = RMSE(model,X,y)\n",
    "rmse_cv = RMSE_CV(model,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Normal equation in Numpy\n",
    "\n",
    "Implement multivariate linear regression using the **normal equation** in `numpy`. Do __not__ use any `sklearn` functions.\n",
    "\n",
    "Perform the following steps:\n",
    "* Split the dataset into training and test set, by including the first 120 samples in the training set and the rest in the test set \n",
    "* Implement fitting of the model to the training data in `numpy` using $\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$\n",
    "* Predict the target values on the test set according to $\\mathbf{\\hat{y}}=\\mathbf{Xw}$\n",
    "* Calculate the RMSE on the test set using $RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2}$\n",
    "\n",
    "You will need these `numpy` functions:\n",
    "* `np.hstack` to concatenate two numpy arrays\n",
    "* `np.matmul` performs matrix multiplication\n",
    "* `matrix.T` transposes the `matrix`\n",
    "* `np.linalg.inv` inverts a matrix\n",
    "\n",
    "Some of the code has been provided for you, fill in the missing commands. You can refer to the Part I of lecture to help you complete this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column of ones\n",
    "x0 = np.ones([X.shape[0],1])\n",
    "\n",
    "# Concatenate with the feature matrix to add feature zero\n",
    "X1 = None\n",
    "\n",
    "# Print the first three lines of the new feature matrix\n",
    "print(np.around(X1[:3,:],2))\n",
    "\n",
    "# Print the new number of features\n",
    "print('\\n The new number of features is ', X1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test set\n",
    "X1_train = X1[:120,:]\n",
    "y_train = None\n",
    "X1_test = None\n",
    "y_test = y[120:]\n",
    "print('Number of samples in train {} and test {}'.format(X1_train.shape[0],X1_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement equation w=(XtX)^-1 * Xty\n",
    "XtX = np.matmul(X1_train.T,None)\n",
    "Xty = None\n",
    "XtXinv = np.linalg.inv(None)\n",
    "w = None\n",
    "print('Coeffs:',np.around(w,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set using y=X*w\n",
    "y_pred_test = None\n",
    "\n",
    "# plot predicted against expected target values\n",
    "plt.scatter(y_test,y_pred_test, label = 'target values')\n",
    "plt.plot([28,44],[28,44],'r', label = '$y=\\hat{y}$')\n",
    "plt.title('Predictions on test set')\n",
    "plt.xlabel('Target values')\n",
    "plt.ylabel('Predicted target values')\n",
    "_=plt.legend(fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE on test set \n",
    "error = None\n",
    "rmse = np.sqrt(np.mean(None))\n",
    "print('RMSE test: {} weeks'.format(round(rmse,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Compare performance of linear regression models with different numbers of features \n",
    "\n",
    "In this exercise we will compare performance of multivariate linear regression models with different numbers of features to predidict age of a baby.\n",
    "\n",
    "First we will now load the datasets and print out number of features. Run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets. Target values y  are the same for all datasets\n",
    "X1,y = CreateFeaturesTargets('datasets/GA-brain-volumes-1-feature.csv')\n",
    "print('Number of features in X1 is ', X1.shape[1])\n",
    "X6,y = CreateFeaturesTargets('datasets/GA-brain-volumes-6-features.csv')\n",
    "print('Number of features in X6 is ', X6.shape[1])\n",
    "X86,y = CreateFeaturesTargets('datasets/GA-brain-volumes-86-features.csv')\n",
    "print('Number of features in X86 is ', X86.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will create a linear regression model and calculate RMSE on the whole set. Use the function `RMSE` that we have created previously. Fill in the code bellow and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Single feature\n",
    "print('Single feature:')\n",
    "rmse1 = None\n",
    "\n",
    "# 6 features\n",
    "print('Six features:')\n",
    "rmse6 = None\n",
    "\n",
    "# 86 features\n",
    "print('86 features:')\n",
    "rmse86 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with RMSE on the whole set as we increase the number of features? Can you interpret this behaviour?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the cross-validated RMSE for the three different feature matrices. You can use function `RMSE_CV` that we created before to calculate and print out the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single feature\n",
    "print('Single feature:')\n",
    "rmse_cv1 = None\n",
    "\n",
    "# 6 features\n",
    "print('Six features:')\n",
    "rmse_cv6 = None\n",
    "\n",
    "# 86 features\n",
    "print('86 features:')\n",
    "rmse_cv86 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model performs the best? Which model is overfitted? \n",
    "\n",
    "__Answer:__ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
